{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to OpenVINO(TM) Toolkit Hands-on Lab Session 1\n",
    "\n",
    "This lab contains; brief notes and resources for OpenVINO(TM), demos, pre-trained models and Model Optimizer samples to practice the tools of OpenVINO(TM) Toolkit. We also have some `python` code samples to understand how we can use Inference Engine API.\n",
    "\n",
    "Jupyter notebook is a browser based IDE and allows partial code executions and text based inputs in markdown format at each cell.\n",
    "\n",
    "Please follow the notations for each instruction in the following sections:\n",
    "\n",
    "- If tutorial indicates you to run a certain command on terminal you will see the text as below and you should open a terminal or continue on the opened one as instructed. \n",
    "\n",
    "**Commands To Run on Terminal**\n",
    "***\n",
    "```bash\n",
    "python3 \n",
    "```\n",
    "***\n",
    "\n",
    "In Ubuntu, when you copy text, easiest way to paste code with keyboard is to press:\n",
    "\n",
    "**SHIFT + INS** buttons\n",
    "\n",
    "- If there are text looks like code; it is mainly the output of an example run of a sample code, please don't copy those back into terminal. \n",
    "\n",
    "**Example Terminal Output** \n",
    "***\n",
    "``` output ```\n",
    "***\n",
    "\n",
    "- If you see python code inside the cell like below, take focus to cell by clicking to it then press:\n",
    "\n",
    "**SHIFT + ENTER** \n",
    "\n",
    "or Click on `>| Run` run button above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cell is a code block example, which you can run inside this browser and see the output directly below the cell. \n",
    "\n",
    "*You don't need to copy the code to terminal or any other IDE.* Just click on `Run` button or change focus to below cell and press `SHIFT + ENTER`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr1 = np.zeros(5)\n",
    "print('Array 1 ', arr1)\n",
    "\n",
    "arr2 = np.ones(5)\n",
    "print('Array 2', arr2)\n",
    "\n",
    "# Array Product \n",
    "print(arr1 * arr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "\n",
    "Here is the topics covered in this Jupyter notebook. This notebook contains introductive information to give you the fundamental knowledge about OpenVINO(TM) Toolkit.\n",
    "\n",
    "## **Part 1** : Introduction to Intel(R) Distribution of OpenVINO(TM) Toolkit\n",
    "\n",
    "A brief introduction about toolkits and software packages of OpenVINO(TM) Toolkit, where you can download and access to documentations. You may skip reading it.\n",
    "\n",
    "\n",
    "## **Part 2** : OpenVINO(TM) Toolkit Demo Application\n",
    "\n",
    "Running OpenVINO(TM) Toolkit demos, classification, object detection, image to text conversion models and understaing flow of OpenVINO(TM) Toolkit enabled application development and run flow.\n",
    "\n",
    "\n",
    "## **Part 3** : Model Zoo and Model Downloader\n",
    "\n",
    "This part will cover the Model Zoo, Intel(R) Pretrained Models and Model Downloader.\n",
    "\n",
    "\n",
    "## **Part 4** : Model Optimizer \n",
    "\n",
    "This part covers the Model Optimizer, how to use it, what kind of optimizations are being used. \n",
    "\n",
    "\n",
    "## **Part 5** : Introduction to Inference Engine API \n",
    "\n",
    "This part includes a Python code sample to work with Inference Engine API for object detection.\n",
    "\n",
    "**Notes**\n",
    "\n",
    "- Output values are only samples, they can vary according to used hardware.\n",
    "- *Other names and brands names, may be claimed as the property of others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: What is Intel(R) Distribution of OpenVINO(TM) Toolkit?\n",
    "\n",
    "Intel(R) Distribution of OpenVINO(TM) Toolkit, OpenVINO(TM) is short for Open Visual Inference and Neural Network Optimization, delivers a set of software packages, libraries and scripts to speed up computer vision and deep learning application development and deployment with existing and your custom deep learning models.\n",
    "\n",
    "Intel(R) Distribution of OpenVINO(TM) Toolkit's main purpose is to speed-up inference, prediction process of Deep Learning models at all Intel(R) Hardware devices, CPU, GPU, VPU and FPGA computing devices.\n",
    "\n",
    "Note that, Intel(R) Distribution of OpenVINO(TM) Toolkit do not help for training deep learning models, enhances only inference part, see https://software.intel.com & https://ai.intel.com sites for other tools intel offers for deep learning.\n",
    "\n",
    "OpenVINO(TM) installation comes with following software tools and libraries. \n",
    "\n",
    "- OpenCV*\n",
    "- OpenVX*\n",
    "- Intel(R) Media SDK\n",
    "- Model Downloader\n",
    "- Model Optimizer\n",
    "- Inference Engine API\n",
    " \n",
    "Model Downloader, Model Optimizer & Inference Engine API are placed under Deep Learning Deployment Toolkit to help Deep Learning deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Intro\n",
    "\n",
    "This part has some brief information about Intel(R) Distribution of OpenVINO(TM) Toolkit to guide you through how to download, get source code and find documentations. \n",
    "\n",
    "All documents of Intel(R) Distribution of OpenVINO(TM) Toolkits has been moved to https://docs.openvinotoolkit.org after R1 2019 release, they are no more available offline.\n",
    "\n",
    "Getting started URL helps you to install and start using OpenVINO(TM) toolkit on your own development environment. \n",
    "\n",
    "- https://docs.openvinotoolkit.org/latest/index.html\n",
    "\n",
    "You can follow installation instructions for Linux, Windows & MacOS* separately. \n",
    "\n",
    "Intel(R) Distribution of OpenVINO(TM) Toolkit helps you to load deeplearning workload to different hardware devices `CPU, GPU, MYRIAD, FPGA` and run inference process on the requested target device.\n",
    "\n",
    "All inference algorithms have been heavily optimized for each hardware platform with corresponding software libraries:\n",
    "\n",
    "- CPU algorithms have been optimized using *Intel(R) Math Kernel Library - DNN* - [mkldnn](https://github.com/intel/mkl-dnn)\n",
    "\n",
    "- GPU operations have been optimized using Intel's OpenCL Library. *openCL - DNN* - [OpenCL DNN](https://github.com/intel/clDNN)\n",
    "\n",
    "- FPGA operations have been optimized with Intel(R) Deep Learning Acceleration Toolkit. \n",
    "\n",
    "During the installation process, do not skip the dependency installation parts, there are multiple dependencies which are not delivered with the installation, so follow the instructions for presquite installation and configuration steps provided in documentations.\n",
    "\n",
    "In Linux distribution, OpenCL library, drivers, python packages and additional software libraries need to be installed to quickly start with OpenVINO(TM). Please do not skip the dependency installation parts when you get started. \n",
    "\n",
    "- https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_linux.html\n",
    "\n",
    "Windows 10 installation process requires additional steps to be completed: Visual Studio Builder installation for CMake builds, paths need to be correctly set and so on.\n",
    "\n",
    "- https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_windows.html\n",
    "\n",
    "Intel(R) Distribution of OpenVINO(TM) Toolkit not only provides software development kits, but also add additional tools to download popular open source Deep Learning models and Intel's own deep learning models to fasten development and deployment of Deep Learning applications. \n",
    "\n",
    "- **Model Downloader** is being used to download popular Deep Learning models developed with Caffe*, Tensorflow*, MxNet*, ONNX* and Kaldi*. Intel models are downloaded during installation and can be easily used. In below sections we will see how they are being used.\n",
    "\n",
    "An additional and required tool for OpenVINO(TM) Toolkit is Model Optimizer which helps you to convert and optimize your model to work with Inference Engine.\n",
    "\n",
    "- **Model Optimizer** is being used to convert frozen models from Caffe*, Tensorflow*, MxNet*, ONNX* and Kaldi* to Intel(R) Distribution of OpenVINO(TM)'s IR files (.xml and .bin) to develop with them. Model Optimizer also provides couple more steps to additional configurations of the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opensource OpenVINO Toolkit\n",
    "\n",
    "OpenVINO(TM) is being integrated to OpenCV* project. Below URL is the main page for the open-source project. \n",
    "\n",
    "- https://01.org/openvinotoolkit \n",
    "\n",
    "Inference Engine and Model Optimizer are being maintained under DLDT project of OpenCV repository. \n",
    "\n",
    "- https://github.com/opencv/dldt \n",
    "\n",
    "Model Downloader and Intel Models are begin maintained under Open Model Zoo repository. Couple of populat Intel Model's (Plate Recognition) can be retrained using the scripts maintained here and also other frozen models can also be accesed from the below URL.\n",
    "\n",
    "- https://github.com/opencv/open_model_zoo \n",
    "\n",
    "Finally, as a complementary for Open Model Zoo, training toolbox is also helpful to retrain couple of popular intel models, this is something under heavy development. \n",
    "\n",
    "- https://github.com/opencv/training_toolbox_tensorflow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Intel(R) Distribution of OpenVINO(TM) Demos\n",
    "\n",
    "In previous part & presentations, resources and some fundamental information about(R) Distribution of OpenVINO(TM) Toolkit has been pased.\n",
    "\n",
    "In this part, we will run the demos delivered with the toolkit to make a quick start with Intel(R) Distribution of OpenVINO(TM) Toolkit. \n",
    "\n",
    "Demo applications of OpenVINO(TM) Toolkit, helps us to validate our installation for the platform. Demo scripts simply builds sample applications from source and run automatically. \n",
    "\n",
    "- OpenVINO(TM) Toolkit demo's has been placed under `/opt/intel/openvino/deployment_tools/demo` folder.\n",
    "\n",
    "These demos are the start points of OpenVINO(TM) Toolkit, they initially checks for dependencies and build the samples.\n",
    "\n",
    "`classification_sample` is the first sample for image classification using SquezeNet\n",
    "`security_barrier_demo` is used to show how Intel(R) models are working. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Demo\n",
    "\n",
    "Follow the instructions below to run the image classification and security barrier demo. \n",
    "\n",
    "Demo script first downloads the squeezenet1.1 Caffe model and then converts it to IR Files. After model conversion is completed `classification_sample` binary loads the model files to run classification on the image shown below. \n",
    "\n",
    "As an output you will only see the classification predictions for the image. \n",
    "\n",
    "![Car](images/car.png) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: Setting up environment variables is required when you work with shared libraries on your deployment environment, OpenVINO(TM) toolkit provides scripts for you to do that. If you start your OpenVINO(TM) Toolkit installation on your development environment you can use below command to setup environment variables for Linux, for Windows 10 you will find `setupvars.bat` file. It is already done for you on this setup so you can skip this part. \n",
    "\n",
    "***\n",
    "```bash\n",
    "source /opt/intel/openvino/bin/setupvars.sh\n",
    "```\n",
    "***\n",
    "Output (Don't Copy to Terminal):\n",
    "***\n",
    "\n",
    "> ```bash [setupvars.sh] OpenVINO environment initialized ```\n",
    "***\n",
    "\n",
    "Let's proceed to run Classification demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN THE COMMANDS BELOW TO RUN CLASSIFICATION DEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can directly run single terminal commands inside Jupyter Notebook Cells, they inherit the existing environment variables defined for this instance Jupyter Notebook Server and use the current directory where this Notebook located.\n",
    "\n",
    "Terminal commands starts with `!` to below is the first command which we will use later in this section. \n",
    "\n",
    "```\n",
    "!echo <sudo pwd> | sudo -S /opt/intel/openvino/deployment_tools/demo/demo_squeezenet_download_convert_run.sh\n",
    "```\n",
    "\n",
    "Above command runs the script to perform below steps and run the delivered image classification demo application of OpenVINO(TM) Toolkit.\n",
    "\n",
    "- Download and install missing dependencies and packages if required\n",
    "\n",
    "- Script will compile the classification sample code to generate binary\n",
    "\n",
    "- Script downloads classification deep learning model\n",
    "\n",
    "- Converts downloaded DL Model to IR file to run with OpenVINO(TM) Toolkit Model Optimizer.\n",
    "\n",
    "- Finally, classification example run with the given arguments.\n",
    "\n",
    "\n",
    "Please carefully check the output of below command after you run to understand running principles.\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!echo intel123 | sudo -S /opt/intel/openvino/deployment_tools/demo/demo_squeezenet_download_convert_run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's Run Script Tasks Manually to Understand OpenVINO(TM) Toolkit Working Principles\n",
    "\n",
    "### 1. Let's See Available Models to Download\n",
    "\n",
    "Following command will list all available models to download.\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --print_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download squeezenet1.1 model for image classification. \n",
    "\n",
    "**Note** If there is no internet connection you can skip this step, model already downloaded on the lab PCs inside `/home/intel/openvino_models` folder.\n",
    "\n",
    "Classification demo starts with downloading the `squeezenet1.1` Caffe model files, `.caffemodel` and `.prototxt`.\n",
    "\n",
    "`.prototxt` file contains deep learning network topology and `.caffemodel` contains the weights and biases.\n",
    "\n",
    "```bash\n",
    "python3 /opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py \\\n",
    "--name squeezenet1.1 \\\n",
    "--output_dir /home/intel/openvino_models/\n",
    "```\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py \\\n",
    "--name squeezenet1.1 \\\n",
    "--output_dir /home/intel/openvino_models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if model downloaded correctly and files are in the destination folder.\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/intel/openvino_models/classification/squeezenet/1.1/caffe/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert Caffemodel to OpenVINO(TM) Toolkit Intermediate Representative Files\n",
    "\n",
    "Inference Engine can't load the existing framework's frozen deep learning model files, they need to be converted to a common repsentative files called IR (Intermediate Representative) files by Model Optimizer.\n",
    "\n",
    "Model Optimizer not only converts the model but also does certain optimization operations to ensure inference execution can be done faster with similar accuracy.\n",
    "\n",
    "Classification demo, converts .caffemodel and .prototxt to .bin and .xml files to make them ready to be loaded by Inference Engine. \n",
    "\n",
    "```bash\n",
    "python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
    "--input_model /home/intel/openvino_models/classification/squeezenet/1.1/caffe/squeezenet1.1.caffemodel \\\n",
    "--output_dir /home/intel/openvino_models/classification/squeezenet/1.1/caffe/FP32\n",
    "```\n",
    "\n",
    "We save converted models to `/home/intel/openvino_models/classification/squeezenet/1.1/caffe/FP32` folder.\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
    "--input_model /home/intel/openvino_models/classification/squeezenet/1.1/caffe/squeezenet1.1.caffemodel \\\n",
    "--output_dir /home/intel/openvino_models/classification/squeezenet/1.1/caffe/FP32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Classification with Converted Model and Image to get Classification Results\n",
    "\n",
    "As you can see from previous commands, we first downloaded the classification demo, then converted it to IR files. Now, we can use this model with classification application to use for image classification. \n",
    "\n",
    "All sample applications are already built inside `/home/intel/inference_engine_samples_build/intel64/Release/` folder. Let's see the applications with below command.\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/intel/inference_engine_samples_build/intel64/Release/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With below command, we will run the `classification_sample` application with given model and car image.\n",
    "\n",
    "- Car image is in `/opt/intel/openvino/deployment_tools/demo` folder.\n",
    "\n",
    "- Model files are in `/home/intel/openvino_models/classification/squeezenet/1.1/caffe/FP32` folder.\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/home/intel/inference_engine_samples_build/intel64/Release/classification_sample \\\n",
    "-i /opt/intel/openvino/deployment_tools/demo/car.png \\\n",
    "-m /home/intel/openvino_models/classification/squeezenet/1.1/caffe/FP32/squeezenet1.1.xml \\\n",
    "–d CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous command used CPU for inference, below command uses GPU to run classification inference.\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/home/intel/inference_engine_samples_build/intel64/Release/classification_sample \\\n",
    "-i /opt/intel/openvino/deployment_tools/demo/car.png \\\n",
    "-m /home/intel/openvino_models/classification/squeezenet/1.1/caffe/FP32/squeezenet1.1.xml \\\n",
    "–d GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With last command we used GPU for inference process and our model data type was FP32. Let's try to use FP16 with GPU. \n",
    "\n",
    "Let's convert to model to FP16 and save it to another folder with below command.\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
    "--input_model /home/intel/openvino_models/classification/squeezenet/1.1/caffe/squeezenet1.1.caffemodel \\\n",
    "--output_dir /home/intel/openvino_models/classification/squeezenet/1.1/caffe/FP16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the command with the FP16 models. \n",
    "\n",
    "**NOTE**: CPU can't use FP16 models, CPU only works with FP32 data type.\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/home/intel/inference_engine_samples_build/intel64/Release/classification_sample \\\n",
    "-i /opt/intel/openvino/deployment_tools/demo/car.png \\\n",
    "-m /home/intel/openvino_models/classification/squeezenet/1.1/caffe/FP16/squeezenet1.1.xml \\\n",
    "–d GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5. Security Barrier Demo\n",
    "\n",
    "Previous section showed us basic steps to complete an inference with Intel(R) Distribution of OpenVINO(TM) Toolkit. There is a second demo which is named as `security_barrier_camera_demo`. \n",
    "\n",
    "This demo application uses models to detect car, plate and convert image to text. Demo opens a window so we need to run this demo on terminal.\n",
    "\n",
    "- **Open a new Terminal Use `CTRL+ALT+T`***\n",
    "\n",
    "Copy below command and paste to terminal (Quick info: Use `SHIFT+INSERT` keys to paste) to go to demo script directory.\n",
    "\n",
    "***\n",
    "```bash\n",
    "cd /opt/intel/openvino/deployment_tools/demo\n",
    "```\n",
    "***\n",
    "\n",
    "Run demo script and type sudo password if required (sudo pwd: intel123)\n",
    "\n",
    "***\n",
    "```bash\n",
    "./demo_security_barrier_camera.sh\n",
    "```\n",
    "***\n",
    "\n",
    "Execution Output:\n",
    "\n",
    "![Sample Car](images/security_camera_demo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Model Zoo & Model Downloader\n",
    "\n",
    "Intel(R) Distribution of OpenVINO(TM) aims to reduce time on development and deployment process for Computer Vision & Deep Learning applications. Therefore, toolkit provides tools to easily download popular publicly available open source deep learning models & a set pre-trained models by Intel(R).\n",
    "\n",
    "### Model Zoo\n",
    "\n",
    "All models are maintained under the given repo:  \n",
    "\n",
    "https://github.com/opencv/open_model_zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Downloader\n",
    "\n",
    "Model Downloader is a Python script stored under `/opt/intel/openvino/deployment_tools/tools/model_downloader` directory.\n",
    "\n",
    "Below is the command to list all available models.\n",
    "\n",
    "Use `-h` command to see all the options of downloader script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "\n",
    "List all available Deep Learning models, which can be downloaded. \n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a list of available models to download with downloader.py script\n",
    "# Press (SHIFT + Enter or Click on Run Button)\n",
    "!python3 /opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --print_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download a public object detection model built by Caffe framework called `MobileNet SSD`, we will use this model for following sessions as well. \n",
    "\n",
    "We will download all the public models and converted IR files to default `home/intel/openvino_models` folder. \n",
    "\n",
    "**Note**: If you don't have internet connection you can skip below part because it requires internet connection to download raw model, which is already available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n",
    "Download MobileNet-SSD Object Detection Model with Below Command.\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there is no internet connection, you can get error, don't worry it is already downloaded.\n",
    "# Press (SHIFT + Enter or Click on Run Button)\n",
    "!python3 /opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py \\\n",
    "--name mobilenet-ssd \\\n",
    "-o /home/intel/openvino_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference Command for Downloading GoogleNet Model. \n",
    "\n",
    "**NOTE: This is just a for your reference, models already downloaded. You don't need to run the commands below**\n",
    "\n",
    "***\n",
    "```bash\n",
    "\n",
    "python3 /opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name googlenet-v4 -o /home/intel/openvino_models\n",
    "\n",
    "###############|| Downloading topologies ||###############\n",
    "\n",
    "========= Downloading /home/intel/openvino_models/classification/googlenet/v4/caffe/googlenet-v4.prototxt\n",
    "... 100%, 84 KB, 243 KB/s, 0 seconds passed\n",
    "\n",
    "========= Downloading /home/intel/openvino_models/classification/googlenet/v4/caffe/googlenet-v4.caffemodel\n",
    "... 100%, 166774 KB, 277 KB/s, 601 seconds passed\n",
    "\n",
    "\n",
    "###############|| Post processing ||##############\n",
    "```\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference Command for Downloading Alexnet \n",
    "\n",
    "**NOTE: This is just a for your reference, models already downloaded. You don't need to run the commands below**\n",
    "\n",
    "***\n",
    "```bash\n",
    "python3 /opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name alexnet -o /home/intel/openvino_models\n",
    "\n",
    "###############|| Downloading topologies ||###############\n",
    "\n",
    "========= Downloading /home/intel/openvino_models/classification/alexnet/caffe/alexnet.prototxt\n",
    "... 100%, 3 KB, 8086 KB/s, 0 seconds passed\n",
    "\n",
    "========= Downloading /home/intel/openvino_models/classification/alexnet/caffe/alexnet.caffemodel\n",
    "... 100%, 238146 KB, 280 KB/s, 847 seconds passed\n",
    "\n",
    "\n",
    "###############|| Post processing ||###############\n",
    "\n",
    "========= Changing input dimensions in alexnet.prototxt =========\n",
    "```\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    "\n",
    "List GoogleNet and AlexNet Model Files\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See that Alexnet and Googlenet is downloaded\n",
    "# Press (SHIFT + Enter or Click on Run Button)\n",
    "!ls /home/intel/openvino_models/classification/alexnet/caffe\n",
    "!ls /home/intel/openvino_models/classification/googlenet/v4/caffe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Model Optimizer\n",
    "\n",
    "Model Optimizer is a cross-platform command-line tool that facilitates the transition between the training and deployment environment, performs static model analysis, and adjusts deep learning models for optimal execution on end-point target devices.\n",
    "\n",
    "Model Optimizer process assumes you have a network model trained using a supported deep learning framework. The scheme below illustrates the typical workflow for deploying a trained deep learning model:\n",
    "\n",
    "![Model Optimizer](images/model_optimizer_schema.png)\n",
    "\n",
    "Model Optimizer used to convert and optimize existing models to IR after certain static topological analysis of existing Caffe*, Tensorflow*, MxNet*, ONNX* and Kaldi* models and perform following optimizations on deep learning networks. \n",
    "\n",
    "- Batch Normalisation\n",
    "- Mean Variance Normalisation\n",
    "- Horizontal Fusion\n",
    "- L2 Normalization Pattern\n",
    "- Constant Folding\n",
    "- Convolutional/Deconvolutional Grouping\n",
    "- Linear Operation Fusion\n",
    "\n",
    "Model Optimizer Developer Guide can be accessed from following URL: https://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Optimizer is a Python script to load Tensorflow*, Caffe*, MxNet*, Kaldi* & ONNX* framework models to convert them to OpenVINO(TM) Toolkit IR files.\n",
    "\n",
    "Since all frameworks uses a different layer names and configurations, for each framework there are different Python scripts.\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /opt/intel/openvino/deployment_tools/model_optimizer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the output of previous command, each framework has its own Python script, however you can also use `mo.py` and determine the framework from command line or it detects it automatically to start conversion.\n",
    "\n",
    "Let's see what kind of options we have with `mo.py` for conversion and optimization process.\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Commands and Optimization Operations\n",
    "\n",
    "There are many options either specific to framework or model conversion, optimizations configurations. A few sample options for Model Optimizer arguments.\n",
    "\n",
    "```\n",
    "--data_type option helps to convert weights and biases to be converted to given floating point type. By default all parametes wil be in FP32 format.\n",
    "\n",
    "here is a very detailed article by Intel(R) https://software.intel.com/en-us/articles/lower-numerical-precision-deep-learning-inference-and-training \n",
    "```\n",
    "\n",
    "```\n",
    "--disable_fusing      Turn off fusing of linear operations to Convolution. During optimization processes, certain operations are fused to dismiss unnecessary operations, so this can be canceled as well. \n",
    "\n",
    "```\n",
    "\n",
    "Please refer to detailed article here for more information about Model Optimizer conversion: https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Converting_Model_General.html\n",
    "\n",
    "We will now proceed to to convert previously downloaded `mobilenet-ssd` Caffe* model to OpenVINO(TM) Toolkit IR files, both for FP16 and FP32 data types.\n",
    "\n",
    "- Please see following URL for detailed optimization steps used by Model Optimizer:\n",
    "\n",
    "https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_Model_Optimization_Techniques.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting Convertion\n",
    "\n",
    "Deep Learning training don't like heterogeneous inputs e.g. 120, 130, -22 , to train, so almost all the time input data is normalized between `[0.0 - 1.0]` to make deep learning model learn more efficiently, this is also valid for image inputs.\n",
    "\n",
    "In many image procesing neural network, inputs are scaled down and RGB values are normalized with mean and scale values, so Model Optimizer should be aware of those values. \n",
    "\n",
    "If you trained your model you should know about the `scale`, if you are using someone else's network those values should be passed to you.\n",
    "\n",
    "Normalization mainly is being done as following. `(IMAGE MATRIX - SUBSTRACT MEAN) / DIVIDE SCALE FACTOR`\n",
    "\n",
    "So, for all the reasons of deep learning model training, `mobilenet-ssd` model scaled with 256 means divided by the largest value to get values between 1.0 and 0.0 so we will use\n",
    "\n",
    "```--scale 256```\n",
    "\n",
    "Mean values are determined as `127` so we will indicate that with `--mean_values` argument.\n",
    "\n",
    "```--mean_values [127,127,127]```\n",
    "\n",
    "Option explanations:\n",
    "\n",
    "- `scale` means : `All input values coming from original network inputs will be divided by this value.`\n",
    "- `mean_values` means: `Mean values to be used for the input image per channel. Values to be provided in the (R,G,B) or [R,G,B] format.`\n",
    "\n",
    "### What happens if I dont' use normalization? \n",
    "\n",
    "Model Optimizer can't infer optimization steps so optimization would not be as expected.\n",
    "\n",
    "\n",
    "### Do I need to Normalize input images while using Inference Engine?\n",
    "\n",
    "Yes, you will normalize input images during the application run time when you loaded the input image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "\n",
    "Let's Convert Caffe Model Files (MobileNet-SSD) to Intel(R) Distribution of OpenVINO(TM) IR Files (.xml , .bin) with data type FP32.\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP 32 Optimization and Convertion\n",
    "# Press (SHIFT + Enter or Click on Run Button)\n",
    "!python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
    "--input_model \"/home/intel/openvino_models/models/object_detection/common/mobilenet-ssd/caffe/mobilenet-ssd.caffemodel\" \\\n",
    "--output_dir /home/intel/openvino_models/models/object_detection/common/mobilenet-ssd/FP32 --data_type FP32 --scale 256 --mean_values [127,127,127]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n",
    "Convert Caffe Model Files (MobileNet-SSD) to Intel(R) Distribution of OpenVINO(TM) IR Files (.xml , .bin) with FP16 Data Type.\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP 16 Optimization and Convertion\n",
    "# Press (SHIFT + Enter or Click on Run Button)\n",
    "!python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
    "--input_model \"/home/intel/openvino_models/models/object_detection/common/mobilenet-ssd/caffe/mobilenet-ssd.caffemodel\" \\\n",
    "--output_dir /home/intel/openvino_models/models/object_detection/common/mobilenet-ssd/FP16 --data_type FP16 --scale 256 --mean_values [127,127,127]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End Note\n",
    "\n",
    "So, we have briefly seen how Model Optimizer to convert existing models trained with Caffe*, Tensorflow*, ONNX*, Kaldi* & MxNet*. Now, we can proceed to use converted model with OpenVINO(TM) Toolkit Inference Engine with Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Inference Engine Python API\n",
    "\n",
    "At this part, we will make a brief introduction to Inference Engine Python API and see how you can implement inference with IE API. \n",
    "\n",
    "C++ API don't change much in logic, so when you get familiar with IE you could implement your own inference easily.\n",
    "\n",
    "We will run each Python code snippet in this browser and see the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python code blocks run inside the cells, in order to run below code, don't copy and paste it to terminal. Change focus to cell and press (SHIFT + ENTER) or click on Run button. \n",
    "\n",
    "#### If you don't see any output after the run of Python code, that means code successfully run inside the cell and you can skip to next cell and execute accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 . Import Required Python Libraries including OpenVINO(TM) Library\n",
    "\n",
    "At this part we will import the libraries required for implementation and certain operations.\n",
    "\n",
    "`cv2` is OpenCV* Library\n",
    "`numpy` is used for vector and numerical operations\n",
    "`matplotlib` is used for plotting operations.\n",
    "\n",
    "In order to successfully import `openvino.inference_engine` environment variables should be set to set `PYTHONPATH`. \n",
    "\n",
    "Let's import libraries:\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Press (SHIFT + Enter or Click on Run Button)\n",
    "# Required Libraries Imported\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Import Required OpenVINO(TM) Libraries\n",
    "from openvino.inference_engine import IEPlugin, IENetwork\n",
    "\n",
    "print('SUCCESS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's control the `PYTHONPATH` to see how we exported environment variables.Show Environment Variables\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Press (SHIFT + Enter or Click on Run Button)\n",
    "# See that PYTHONPATH includes Intel CV Python path which includes OpenVINO Python Modules\n",
    "!echo $PYTHONPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. IEPlugin\n",
    "\n",
    "First thing with Inference Engine is allocating memory and loading required library for the target device. \n",
    "\n",
    "Following method show us the steps to load Inference Engine target library which includes neural network layers and operations to run defined topolgy on the device.\n",
    "\n",
    "The following cell constructs `IEPlugin` object, this is only the method, we will call this method in later section.\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPlugin(target_device, extension_list):\n",
    "    # Plugin initialization for specified device. We will be targeting CPU initially.\n",
    "    plugin = IEPlugin(device=target_device)\n",
    "\n",
    "    # Loading additional kernel extension libraries for the CPU\n",
    "    if target_device == 'CPU':\n",
    "        for extension in extension_list:\n",
    "            plugin.add_cpu_extension('/home/intel/inference_engine_samples_build/intel64/Release/lib/libcpu_extension.so')\n",
    "\n",
    "    return plugin\n",
    "\n",
    "print('SUCCESS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. IENetwork\n",
    "\n",
    "Second thing, we load defined Deep Learning Model with given plugin. Next method shows us how we load the neural network using IR files.\n",
    "\n",
    "We will use this method in later sections.\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Press (SHIFT + Enter or Click on Run Button)\n",
    "def createNetwork(model_xml, model_bin, plugin):\n",
    "    \n",
    "    # Importing network weights from IR models.\n",
    "    net = IENetwork(model=model_xml, weights=model_bin)\n",
    "\n",
    "    # Some layers in IR models may be unsupported by some plugins so we check if there any unsupported layers\n",
    "    if \"CPU\" in plugin.device:\n",
    "        supported_layers = plugin.get_supported_layers(net)\n",
    "        not_supported_layers = [l for l in net.layers.keys() if l not in supported_layers]\n",
    "        if len(not_supported_layers) != 0:\n",
    "            print(\"Following layers are not supported by the plugin for specified device {}:\\n {}\".\n",
    "                      format(plugin.device, ', '.join(not_supported_layers)))\n",
    "            return None\n",
    "    return net\n",
    "\n",
    "print('SUCCESS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ExecutableNetwork\n",
    "\n",
    "When IEPlugin and IENetwork loaded, ExecutableNetwork object used to create inference requests on the network and target device. This is an intermediate access to IENetwork and target device. \n",
    "\n",
    "You will see `input_blob` and `output_blob` objects. Which defines the input shape and output shape of the networks and memory allocations for inputs and outputs.\n",
    "\n",
    "`input_blob` shows us in what shape our inputs should be. \n",
    "`output_blob` determines the output of inference. \n",
    "\n",
    "We will use this method later to generate Executable Network to run inference.\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Press (SHIFT + Enter or Click on Run Button)\n",
    "def loadNetwork(plugin, net):\n",
    "    # Loading IR model to the plugin.\n",
    "    exec_net = plugin.load(network=net, num_requests=2)\n",
    "    \n",
    "    # Getting the input and outputs of the network\n",
    "    input_blob = next(iter(net.inputs))\n",
    "    \n",
    "    out_blob = next(iter(net.outputs))\n",
    "    \n",
    "    return exec_net,input_blob,out_blob\n",
    "\n",
    "print('SUCCESS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Input Pre-processing\n",
    "\n",
    "In previous parts, we have created methods to provide required objects to run inference. However, we also need pre-processing of images, re-sizing the input image. \n",
    "\n",
    "Since, we already determined the normalisation during the model optimization, there only need a shape transformation of the input image. Below, method only helps us to do transformation and resize.\n",
    "\n",
    "We will use this method to resize input images and allocate `input_blob` object to feed into `Executable Network` to get inference values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Press (SHIFT + Enter or Click on Run Button)\n",
    "def preprocessImage(img_path, net, input_blob):\n",
    "    # Reading the frame from a jpeg file\n",
    "    frame = cv.imread(img_path)\n",
    "    \n",
    "    # Reshaping data\n",
    "    n, c, h, w = net.inputs[input_blob].shape\n",
    "    in_frame = cv.resize(frame, (w, h))\n",
    "    in_frame = in_frame.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n",
    "    \n",
    "    return in_frame.reshape((n, c, h, w)),frame\n",
    "\n",
    "print('SUCCESS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets import helper method to draw rectangles and write text to image.\n",
    "\n",
    "Then, the labels of `mobilenet-ssd` model output results maps to given text values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.out_process import placeBoxes\n",
    "\n",
    "labels_map = {0:'background',1: 'aeroplane', 2: 'bicycle', 3: 'bird', 4: 'boat', 5: 'bottle', 6: 'bus', 7: 'car', 8: 'cat',\n",
    "              9: 'chair', 10: 'cow', 11: 'diningtable', 12: 'dog', 13: 'horse', 14: 'motorbike', 15: 'person', 16: 'pottedplant',\n",
    "              17: 'sheep', 18: 'sofa', 19: 'train', 20: 'tvmonitor' }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference\n",
    "\n",
    "Let's do the inference. \n",
    "\n",
    "Below, method helps us to define inference process from start to end. \n",
    "\n",
    "- Load IEPlugin, IENetwork, ExecutableNetwork with provided IR and Target.\n",
    "\n",
    "- Pre-process Input Image\n",
    "\n",
    "- Send image to ExecutableNetwork and call `Infer`\n",
    "\n",
    "- Print the inferred image.\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runInference(device='CPU', \n",
    "                 model_xml='/home/intel/openvino_models/models/object_detection/common/mobilenet-ssd/FP32/mobilenet-ssd.xml', \n",
    "                 model_bin='/home/intel/openvino_models/models/object_detection/common/mobilenet-ssd/FP32/mobilenet-ssd.bin',\n",
    "                 image_file='images/car.png',\n",
    "                 confidence_threshold=0.6):\n",
    "    # Get Plugin\n",
    "    plugin = createPlugin(target_device=device, extension_list=['/home/intel/inference_engine_samples_build/intel64/Release/lib/libcpu_extension.so'])\n",
    "    \n",
    "    # Get Network\n",
    "    net = createNetwork(model_xml, model_bin, plugin)\n",
    "    \n",
    "    # Get Executable Network, Input and Output Layer Information\n",
    "    exec_net,input_blob,out_blob = loadNetwork(plugin, net)\n",
    "    \n",
    "    # Pre-process Image According to Input Layer\n",
    "    in_frame,original_frame = preprocessImage(image_file, net, input_blob)\n",
    "\n",
    "    # Starting the inference in async mode, which starts the inference in parallel\n",
    "    inference_start = time.time()\n",
    "    \n",
    "    # Start Infering given Frame/Image\n",
    "    exec_net.infer(inputs={input_blob: in_frame})\n",
    "    \n",
    "    # Getting the result of the network\n",
    "    res = exec_net.requests[0].outputs[out_blob]\n",
    "    inference_end = time.time()\n",
    "    # Processing the output result and adding labels on the image. Implementation is not shown in the\n",
    "    #  this notebook; you can find it in object_detection_demo_ssd_async.py\n",
    "    initial_w = original_frame.shape[1]\n",
    "    initial_h = original_frame.shape[0]\n",
    "\n",
    "    # Out Process/Draw Boxes\n",
    "    frame = placeBoxes(res, labels_map, confidence_threshold, original_frame, initial_w, initial_h, False, 0, ((inference_end - inference_start)))\n",
    "    # We use pyplot because it plays nicer with Jupyter Notebooks\n",
    "    fig = plt.figure(dpi=300)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(cv.cvtColor(frame, cv.COLOR_BGR2RGB), interpolation='none')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "        \n",
    "    return None\n",
    "\n",
    "print('SUCCESS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out CPU information\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Press (SHIFT + Enter or Click on Run Button)\n",
    "!cat /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference with CPU\n",
    "\n",
    "Since model folder and confidence default values defined, here we just want to run on CPU. You can change confidence threshold and try with different images to see the output.\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CPU Inference\n",
    "runInference('CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see the GPU on the system run the following command on terminal\n",
    "\n",
    "```bash\n",
    "lspci -nn -s 0:02.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inference with GPU\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Press (SHIFT + Enter or Click on Run Button)\n",
    "!lspci -nn -s 0:02.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Inference on GPU with FP32 Converted Model\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Press (SHIFT + Enter or Click on Run Button)\n",
    "runInference(\"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Inference on GPU with FP16 Converted Model\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Press (SHIFT + Enter or Click on Run Button)\n",
    "runInference(device='GPU', \n",
    "             model_xml='/home/intel/openvino_models/models/object_detection/common/mobilenet-ssd/FP16/mobilenet-ssd.xml', \n",
    "             model_bin='/home/intel/openvino_models/models/object_detection/common/mobilenet-ssd/FP16/mobilenet-ssd.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please feel free to try above code with different input images, confidence threshold or if you are comfortable with OpenCV and Python try to customize application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOLLOW UP SESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Security Barrier Follow Up\n",
    "\n",
    "You can see the execution commands from the script outputs. You can change them and try to run with different images, target platroms etc. \n",
    "\n",
    "Below command will run `security_barrier_camera_demo` on GPU, we will also go over a similar example with multiple models and multiple targets. \n",
    "\n",
    "```bash\n",
    "cd /home/intel/inference_engine_samples_build/intel64/Release/\n",
    "\n",
    "./security_barrier_camera_demo -d GPU -d_va GPU -d_lpr GPU -i /opt/intel/openvino/deployment_tools/demo/car_1.bmp -m /home/intel/openvino_models/ir/FP32/Security/object_detection/barrier/0106/dldt/vehicle-license-plate-detection-barrier-0106.xml -m_va //home/intel/openvino_models/ir/FP32/Security/object_attributes/vehicle/resnet10_update_1/dldt/vehicle-attributes-recognition-barrier-0039.xml -m_lpr /home/intel/openvino_models/ir/FP32/Security/optical_character_recognition/license_plate/dldt/license-plate-recognition-barrier-0001.xml\n",
    "\n",
    "[ INFO ] InferenceEngine: \n",
    "\tAPI version ............ 1.4\n",
    "\tBuild .................. 19154\n",
    "[ INFO ] Parsing input parameters\n",
    "[ INFO ] Capturing video streams from the video files or loading images\n",
    "[ INFO ] Files were added: 1\n",
    "[ INFO ]     /opt/intel/openvino/deployment_tools/demo/car_1.bmp\n",
    "[ INFO ] Number of input image files: 1\n",
    "[ INFO ] Number of input video files: 0\n",
    "[ INFO ] Number of input channels:    1\n",
    "[ INFO ] Loading plugin GPU\n",
    "\n",
    "\tAPI version ............ 1.5\n",
    "\tBuild .................. 19154\n",
    "\tDescription ....... clDNNPlugin\n",
    "[ INFO ] Loading network files for VehicleDetection\n",
    "[ INFO ] Batch size is forced to  1\n",
    "[ INFO ] Checking Vehicle Detection inputs\n",
    "[ INFO ] Checking Vehicle Detection outputs\n",
    "[ INFO ] Loading Vehicle Detection model to the GPU plugin\n",
    "[ INFO ] Loading network files for VehicleAttribs\n",
    "[ INFO ] Batch size is forced to 1 for Vehicle Attribs\n",
    "[ INFO ] Checking VehicleAttribs inputs\n",
    "[ INFO ] Checking Vehicle Attribs outputs\n",
    "[ INFO ] Loading Vehicle Attribs model to the GPU plugin\n",
    "[ INFO ] Loading network files for Licence Plate Recognition (LPR)\n",
    "[ INFO ] Batch size is forced to  1 for LPR Network\n",
    "[ INFO ] Checking LPR Network inputs\n",
    "[ INFO ] Checking LPR Network outputs\n",
    "[ INFO ] Loading LPR model to the GPU plugin\n",
    "[ INFO ] Start inference \n",
    "\n",
    "Avarage inference time: 27.3022 ms (36.6271 fps)\n",
    "\n",
    "Total execution time: 11845.8\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building All Samples\n",
    "\n",
    "This is an extra stage, which is already done for you, but for future reference it is good to have all the commands.\n",
    "\n",
    "Let's get into home directory.\n",
    "\n",
    "***\n",
    "```bash \n",
    "cd /home/intel\n",
    "```\n",
    "***\n",
    "\n",
    "If `inference_engine_samples_build` folder exists, **remove** and remake it.\n",
    "\n",
    "***\n",
    "```bash\n",
    "rm -rf /home/intel/inference_engine_samples_build && mkdir /home/intel/inference_engine_samples_build\n",
    "```\n",
    "***\n",
    "\n",
    "***\n",
    "```bash\n",
    "cd /home/intel/inference_engine_samples_build\n",
    "```\n",
    "***\n",
    "\n",
    "Then, we will recreate build files in this folder. \n",
    "\n",
    "***\n",
    "```bash\n",
    "source /opt/intel/openvino/bin/setupvars.sh\n",
    "```\n",
    "***\n",
    "\n",
    "***\n",
    "```bash\n",
    "cmake /opt/intel/openvino/deployment_tools/inference_engine/samples/\n",
    "```\n",
    "***\n",
    "\n",
    "***\n",
    "```bash\n",
    "make all -j8\n",
    "```\n",
    "***\n",
    "\n",
    "After, successfull make all the sample applications will be ready to use in `/home/intel/inference_engine_samples/intel64/Release` folder.\n",
    "\n",
    "***\n",
    "```bash\n",
    "ls /home/intel/inference_engine_samples_build/intel64/Release\n",
    "```\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
