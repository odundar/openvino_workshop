{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to OpenVINO(TM) Hands-on Lab Session 3 - Advanced Features\n",
    "\n",
    "In this tutorial, we will look at some of the advanced featues of Intel(R) Distribution of OpenVINO(TM) to help you get performance upgrades, using multiple target devices at the same time, creating custom layers and configuring execution of layers manually on different hardware devices.\n",
    "\n",
    "Jupyter notebook is a browser based IDE and allows partial code executions and text based inputs as markdown at each cell.\n",
    "\n",
    "Please follow the notations for each instruction in the following sections:\n",
    "\n",
    "- If tutorial wants you run a certain command on terminal you will see the text as below. You should open a terminal or continue on the opened one as instructed. \n",
    "\n",
    "**Commands To Run on Terminal**\n",
    "***\n",
    "```bash\n",
    "python3 \n",
    "```\n",
    "***\n",
    "\n",
    "In Ubuntu, when you copy text, easiest way to paste code with keyboard is to press:\n",
    "\n",
    "**SHIFT + INS** buttons\n",
    "\n",
    "- If there are text looks like code; it is mainly the output of an example run of a sample code, please don't copy those back into terminal. \n",
    "\n",
    "**Example Terminal Output** \n",
    "***\n",
    "``` output ```\n",
    "***\n",
    "\n",
    "- If you see python code inside the cell like below, take focus to cell by clicking to it then press:\n",
    "\n",
    "**SHIFT + ENTER** \n",
    "\n",
    "or Click on `>| Run` run button above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below example is a code block, which you can run inside this browser session and see the output directly below the cell. You don't need to copy the code to terminal or any other IDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr1 = np.zeros(5)\n",
    "print('Array 1 ', arr1)\n",
    "\n",
    "arr2 = np.ones(5)\n",
    "print('Array 2', arr2)\n",
    "\n",
    "# Array Product \n",
    "print(arr1 * arr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "\n",
    "## Part 1 - Inference Engine - Performance Counters\n",
    "Another feature of Inference Engine will be seen at this part to see performance metrics of layer executions.\n",
    "\n",
    "\n",
    "## Part 2 - Inference Engine - Heterogeneous Plugin\n",
    "At this section we will run Inference Engine Heterogeneous Plugin code samples inside Jupyter Notebook to see how Heterogeneous Plugin runs on heterogeneous hardware environments. \n",
    "\n",
    "\n",
    "## Part 3 - Inference Engine - Layer Affinity\n",
    "This part is a follow-up of Heterogeneous Plugin which shows how manual configuration be applied to DL layer assignment to HW devices.\n",
    "\n",
    "## Part 4 - 8-bit Integer Inference\n",
    "\n",
    "\n",
    "## Part 5 - Custom Layers\n",
    "\n",
    "\n",
    "## Part 6 - Calibration Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Intel Distribution of OpenVINO : Heterogenous Plugin\n",
    "\n",
    "At this session, we would like to go over more detailed explanation of OpenVINO Inference Engine's Heterogeneous API which helps to run inference on heterogeneous platforms. If you are running on a platform with Intel CPU and GPU, you can control what layers of Deep Learning Model would be running on the selected hardware platform. \n",
    "\n",
    "Heterogeneous API has been developed first with fallback principle, it means we wanted to execute layers and methods with a prioritized order. Not all layer's implementations are complete for each platform and certain layers can execute a lot faster on different platforms where some can't. As such, there has been great progress over the heterogeneous API use cases for Inference to analyze the performance bottlenecks and improve the inference process. \n",
    "\n",
    "At this section, we want to showcase its use on object detection scenario and get detailed analysis of executions of layers on devices a quick course of Heterogeneous Plugin of Intel OpenVINO.  \n",
    "\n",
    "Let's start importing required libraries for this session as following cell.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "\n",
    "Implement Helper Methods and Load Libraries\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code with : (SHIFT + ENTER) or Press Run Button #\n",
    "# Let's Import Required Libraries first\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# Import OpenVINO\n",
    "from openvino.inference_engine import IENetwork, IEPlugin\n",
    "\n",
    "# Define Methods\n",
    "\n",
    "def createNetwork(model_xml, model_bin, plugin):\n",
    "    # Importing network weights from IR models.\n",
    "\n",
    "    net = IENetwork(model=model_xml, weights=model_bin)\n",
    "        \n",
    "    return net\n",
    "\n",
    "def loadNetwork(plugin, net, num_requests=2):\n",
    "    # Loading IR model to the plugin.\n",
    "    exec_net = plugin.load(network=net, num_requests=num_requests)\n",
    "    \n",
    "    # Getting the input and outputs of the network\n",
    "    input_blob = next(iter(net.inputs))\n",
    "    out_blob = next(iter(net.outputs))\n",
    "    \n",
    "    return exec_net,input_blob,out_blob\n",
    "\n",
    "def preprocessImage(img_path, net, input_blob):\n",
    "    # Reading the frame from a jpeg file\n",
    "    frame = cv.imread(img_path)\n",
    "    \n",
    "    # Reshaping data\n",
    "    n, c, h, w = net.inputs[input_blob].shape\n",
    "    in_frame = cv.resize(frame, (w, h))\n",
    "    in_frame = in_frame.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n",
    "    \n",
    "    return in_frame.reshape((n, c, h, w)),frame\n",
    "\n",
    "print('SUCCESS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this part, we are implementing a new inference method which is working with heterogeneous plugin. \n",
    "\n",
    "There are couple more steps we have added to inference process.\n",
    "\n",
    "- First we check if there is a CPU device on the list of devices, we load cpu extensions library.\n",
    "\n",
    "- Then, we load the network as usual process. \n",
    "\n",
    "-  `set_config` function configures the plugin fallback devices and their order.\n",
    "\n",
    "Heterogeneous Plugin used with `HETERO:` prefix, which is followed with a list of devices, according to their prioritization. \n",
    "\n",
    "- After network load, we send the network to plugin to set affinity of layers according to priority list we give to plugin. When affinity sets the layer's target device to be executed.\n",
    "\n",
    "- Additionally we have performance counters or certain outputs we can later analyze the network. `.set_config({\"HETERO_DUMP_GRAPH_DOT\": \"YES\"})` prints .dot output which has a detailed graph representation of model. \n",
    "\n",
    "- Finally, `get_perf_counts` method prints the detailed execution times of layers on the devices.\n",
    "\n",
    "Let's run the next cell and make `runInference` method ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n",
    "Implement `runInference` Method which uses Hetero Plugin by Default\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code with : (SHIFT + ENTER) or Press Run Button #\n",
    "\n",
    "# For labeling the image after inference.\n",
    "from out_process import placeBoxes\n",
    "\n",
    "# At this stage we implement our inference method to run with Heterogeneous plugin. \n",
    "\n",
    "def runInference(hetero_device = 'HETERO:GPU,CPU',\n",
    "                 model_xml='/home/intel/openvino_models/ir/mobilenet-ssd/FP32/mobilenet-ssd.xml',\n",
    "                 model_bin='/home/intel/openvino_models/ir/mobilenet-ssd/FP32/mobilenet-ssd.bin',\n",
    "                 image_file='images/car.png',\n",
    "                 performance_counters = False,\n",
    "                 dot_graph = True,\n",
    "                 confidence_threshold=0.6):\n",
    "\n",
    "    # Plugin initialization for specified device. We will be targeting CPU initially.\n",
    "    plugin = IEPlugin(device=hetero_device, plugin_dirs='/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64')\n",
    "    \n",
    "    # Heterogeneous Plugin is provided as\n",
    "    #HETERO:GPU,CPU or similar.\n",
    "    vals = hetero_device.split(':')\n",
    "    targets = vals[1].split(',')\n",
    "    \n",
    "    # Let's check if heterogeneous plugin has CPU target so we can add cpu extensions\n",
    "    # Loading additional exension libraries for the CPU\n",
    "    if hetero_device == 'CPU' or ('CPU' in targets):\n",
    "        extension_list=['/home/intel/inference_engine_samples_build/intel64/Release/lib/libcpu_extension.so']\n",
    "        for extension in extension_list:\n",
    "            plugin.add_cpu_extension(extension)\n",
    " \n",
    "    net = createNetwork(model_xml, model_bin, plugin)\n",
    "    \n",
    "     # Let's check if the target is heterogeneous then we add fallback device priorities\n",
    "     #If we set TARGET_FALLBACK configuration, we want IE to decide what device to fallback. \n",
    "    \n",
    "    plugin.set_initial_affinity(net)\n",
    "    plugin.set_config({\"TARGET_FALLBACK\": hetero_device})\n",
    "    \n",
    "    if dot_graph:\n",
    "        # Just for a show case, remove\n",
    "        plugin.set_config({\"HETERO_DUMP_GRAPH_DOT\": \"YES\"})\n",
    "    \n",
    "    exec_net, input_blob, out_blob = loadNetwork(plugin, net)\n",
    "    \n",
    "    in_frame,original_frame = preprocessImage(image_file, net, input_blob)\n",
    "\n",
    "    my_request_id = 0\n",
    "\n",
    "        # Starting the inference in async mode, which starts the inference in parallel\n",
    "    inference_start = time.time()\n",
    "    exec_net.start_async(request_id=my_request_id, inputs={input_blob: in_frame})\n",
    "    # ... You can do additional processing or latency masking while we wait ...\n",
    "\n",
    "    # Blocking wait for a particular request_id\n",
    "    if exec_net.requests[my_request_id].wait(-1) == 0:\n",
    "        # getting the result of the network\n",
    "        res = exec_net.requests[my_request_id].outputs[out_blob]\n",
    "        inference_end = time.time()\n",
    "        # Processing the output result and adding labels on the image. Implementation is not shown in the\n",
    "        #  this notebook; you can find it in object_detection_demo_ssd_async.py\n",
    "        initial_w = original_frame.shape[1]\n",
    "        initial_h = original_frame.shape[0]\n",
    "\n",
    "        frame = placeBoxes(res, None, confidence_threshold, original_frame, initial_w, initial_h, False, my_request_id, ((inference_end - inference_start)))\n",
    "        # We use pyplot because it plays nicer with Jupyter Notebooks\n",
    "        fig = plt.figure(dpi=300)\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.imshow(cv.cvtColor(frame, cv.COLOR_BGR2RGB), interpolation='none')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"There was an error with the request\")\n",
    "    \n",
    "    if performance_counters:\n",
    "        perf_counts = exec_net.requests[0].get_perf_counts()\n",
    "        print(\"Performance counters:\")\n",
    "        for layer, stats in perf_counts.items():\n",
    "            print(layer, ': ', stats)\n",
    "    \n",
    "    return (plugin, net, exec_net)\n",
    "\n",
    "print('SUCCESS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In next cell, I would like to use Heterogeneous plugin to run object detection sample. \n",
    "\n",
    "With `HETERO:GPU,CPU,MYRIAD` we indicate to our plugin to prioritize `GPU > CPU > MYRIAD` for layers to be executed. \n",
    "\n",
    "For MobileNet-SSD example, almost all layers can run on GPU except `PriorBox` Caffe layer. Therefore, it will use GPU for all the `Convolution` layers and CPU for PriorBox layer.\n",
    "\n",
    "If, we have been used `HETERO:GPU,MYRIAD,CPU` , `PriorBox` layer would be running on MYRIAD since it has also support for it. \n",
    "\n",
    "Note that, we use FP16 because, in case CPU target is being used, IE helps to convert FP16 layers to FP32 automatically.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    "\n",
    "Use Hetero Plugin with GPU, CPU order.\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code with : (SHIFT + ENTER) or Press Run Button #\n",
    "\n",
    "hetero_objects = runInference(hetero_device = 'HETERO:GPU,CPU',\n",
    "                              model_xml='/home/intel/openvino_models/ir/mobilenet-ssd/FP16/mobilenet-ssd.xml',\n",
    "                              model_bin='/home/intel/openvino_models/ir/mobilenet-ssd/FP16/mobilenet-ssd.bin',\n",
    "                              image_file='images/car.png',\n",
    "                              performance_counters = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Layer Affinity\n",
    "\n",
    "Let's first see the affinity of the layers. \n",
    "\n",
    "Below example shows that, Input layer uses CPU and ScaleShift uses GPU.\n",
    "\n",
    "`\n",
    "Type:  Input Device:  CPU\n",
    "Type:  ScaleShift Device:  GPU\n",
    "Type:  Convolution Device:  GPU\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "\n",
    "Get Details from Previous Section for Heterogeneour Workload\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code with : (SHIFT + ENTER) or Press Run Button #\n",
    "\n",
    "net = hetero_objects[1]\n",
    "\n",
    "for l in net.layers.values():\n",
    "    print('Type: ', l.type, 'Device: ', l.affinity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`{\"HETERO_DUMP_GRAPH_DOT\": \"YES\"}` configuraion let's us to print the network visualisation in .dot graph format. You can navigate to current directory and run it as below from a new terminal.\n",
    "\n",
    "\n",
    "```bash\n",
    "!xdot hetero_affinity_MobileNet-SSD.dot\n",
    "```\n",
    "\n",
    "![Dot Output](images/affinity_dot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n",
    "Run implemented inference with given parameters.\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code with : (SHIFT + ENTER) or Press Run Button #\n",
    "\n",
    "hetero_objects = runInferenceBatch(hetero_device = 'HETERO:CPU,GPU',\n",
    "                                   model_xml='/home/intel/openvino_models/ir/mobilenet-ssd/FP16/mobilenet-ssd.xml',\n",
    "                                   model_bin='/home/intel/openvino_models/ir/mobilenet-ssd/FP16/mobilenet-ssd.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    "\n",
    "Show Layer Affinities\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code with : (SHIFT + ENTER) or Press Run Button #\n",
    "\n",
    "net = hetero_objects[1]\n",
    "\n",
    "# This time all CPU, no need to fallback to GPU\n",
    "\n",
    "for l in net.layers.values():\n",
    "    print('Type: ', l.type, 'Device: ', l.affinity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\n",
    "\n",
    "Show Corresponding Performance Counters\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code with : (SHIFT + ENTER) or Press Run Button #\n",
    "\n",
    "# Get the exec net\n",
    "exec_net = hetero_objects[2]\n",
    "\n",
    "# Print the performance counteres\n",
    "perf_counts = exec_net.requests[0].get_perf_counts()\n",
    "print(\"Performance counters:\")\n",
    "for layer, stats in perf_counts.items():\n",
    "    print(layer, ': ', stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7: Performance Counters\n",
    "\n",
    "Let's see the detailed run of execution of layers. Following command will give us the layer executions in details. \n",
    "\n",
    "Note: If GPU used, performance counters returns blank. It doens't report for GPU at this time, see next example for detailed execution report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "\n",
    "See the Performance Counter Details\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code with : (SHIFT + ENTER) or Press Run Button # \n",
    "\n",
    "# Get the exec net\n",
    "exec_net = hetero_objects[2]\n",
    "\n",
    "# Print the performance counteres\n",
    "perf_counts = exec_net.requests[0].get_perf_counts()\n",
    "print(\"Performance counters:\")\n",
    "for layer, stats in perf_counts.items():\n",
    "    print(layer, ': ', stats)\n",
    "\n",
    "# Note: GPU Perfomance Counters Don't Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n",
    "Instructions to Install NCS2 Driver for Ubuntu\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try out:\n",
    "\n",
    "`'HETERO:MYRIAD,GPU,CPU'` and so on. \n",
    "\n",
    "Note: if you get NCS2 Not Found Error you install NCS2 as below. \n",
    "\n",
    "\n",
    "```bash\n",
    "cat <<EOF > 97-usbboot.rules\n",
    "SUBSYSTEM==\"usb\", ATTRS{idProduct}==\"2150\", ATTRS{idVendor}==\"03e7\", GROUP=\"users\", MODE=\"0666\", ENV{ID_MM_DEVICE_IGNORE}=\"1\"\n",
    "SUBSYSTEM==\"usb\", ATTRS{idProduct}==\"2485\", ATTRS{idVendor}==\"03e7\", GROUP=\"users\", MODE=\"0666\", ENV{ID_MM_DEVICE_IGNORE}=\"1\"\n",
    "SUBSYSTEM==\"usb\", ATTRS{idProduct}==\"f63b\", ATTRS{idVendor}==\"03e7\", GROUP=\"users\", MODE=\"0666\", ENV{ID_MM_DEVICE_IGNORE}=\"1\"\n",
    "EOF\n",
    "```\n",
    "\n",
    "```bash\n",
    "sudo cp 97-usbboot.rules /etc/udev/rules.d/\n",
    "sudo udevadm control --reload-rules\n",
    "sudo udevadm trigger\n",
    "sudo ldconfig\n",
    "rm 97-usbboot.rules\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    "\n",
    "Clean Previous Run\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code with : (SHIFT + ENTER) or Press Run Button #\n",
    "\n",
    "# Before re-run delete objects\n",
    "del hetero_objects\n",
    "\n",
    "print('SUCCESS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\n",
    "\n",
    "Re-run with CPU, GPU priority order\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code with : (SHIFT + ENTER) or Press Run Button #\n",
    "\n",
    "hetero_objects = runInference(hetero_device = 'HETERO:CPU,GPU',\n",
    "                              model_xml='/home/intel/openvino_models/ir/mobilenet-ssd/FP16/mobilenet-ssd.xml',\n",
    "                              model_bin='/home/intel/openvino_models/ir/mobilenet-ssd/FP16/mobilenet-ssd.bin',\n",
    "                              image_file='images/car.png',\n",
    "                              performance_counters = False)\n",
    "\n",
    "# Get the exec net\n",
    "exec_net = hetero_objects[2]\n",
    "\n",
    "# Print the performance counteres\n",
    "perf_counts = exec_net.requests[0].get_perf_counts()\n",
    "print(\"Performance counters:\")\n",
    "for layer, stats in perf_counts.items():\n",
    "    print(layer, ': ', stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\n",
    "\n",
    "Show Layer Affinities when Order Changed\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code with : (SHIFT + ENTER) or Press Run Button #\n",
    "\n",
    "net = hetero_objects[1]\n",
    "\n",
    "# This time all CPU, no need to fallback to GPU\n",
    "\n",
    "for l in net.layers.values():\n",
    "    print('Type: ', l.type, 'Device: ', l.affinity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Affinity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heterogeneous Plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INT8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thread Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Yolo for Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
