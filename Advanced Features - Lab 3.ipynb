{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to OpenVINO(TM) Hands-on Lab Session 3 - Advanced Features\n",
    "\n",
    "In this tutorial, we will look at some of the advanced featues of Intel(R) Distribution of OpenVINO(TM) to help you get performance upgrades, using multiple target devices at the same time, creating custom layers and configuring execution of layers manually on different hardware devices.\n",
    "\n",
    "Jupyter notebook is a browser based IDE and allows partial code executions and text based inputs as markdown at each cell.\n",
    "\n",
    "Please follow the notations for each instruction in the following sections:\n",
    "\n",
    "- If tutorial wants you run a certain command on terminal you will see the text as below. You should open a terminal or continue on the opened one as instructed. \n",
    "\n",
    "**Commands To Run on Terminal**\n",
    "***\n",
    "```bash\n",
    "python3 \n",
    "```\n",
    "***\n",
    "\n",
    "In Ubuntu, when you copy text, easiest way to paste code with keyboard is to press:\n",
    "\n",
    "**SHIFT + INS** buttons\n",
    "\n",
    "- If there are text looks like code; it is mainly the output of an example run of a sample code, please don't copy those back into terminal. \n",
    "\n",
    "**Example Terminal Output** \n",
    "***\n",
    "``` output ```\n",
    "***\n",
    "\n",
    "- If you see python code inside the cell like below, take focus to cell by clicking to it then press:\n",
    "\n",
    "**SHIFT + ENTER** \n",
    "\n",
    "or Click on `>| Run` run button above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below example is a code block, which you can run inside this browser session and see the output directly below the cell. You don't need to copy the code to terminal or any other IDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr1 = np.zeros(5)\n",
    "print('Array 1 ', arr1)\n",
    "\n",
    "arr2 = np.ones(5)\n",
    "print('Array 2', arr2)\n",
    "\n",
    "# Array Product \n",
    "print(arr1 * arr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "\n",
    "## Part 1 - Inference Engine - Heterogeneous Plugin\n",
    "At this section we will run Inference Engine Heterogeneous Plugin code samples inside Jupyter Notebook to see how Heterogeneous Plugin runs on heterogeneous hardware environments. \n",
    "\n",
    "## Part 2 - Inference Engine - Layer Affinity\n",
    "This part is a follow-up of Heterogeneous Plugin which shows how manual configuration be applied to DL layer assignment to HW devices.\n",
    "\n",
    "## Part 3 - Inference Engine - Performance Counters\n",
    "Another feature of Inference Engine will be seen at this part to see performance metrics of layer executions.\n",
    "\n",
    "\n",
    "## Part 4 - Custom Layers\n",
    "Custom layers are NN (Neural Network) layers that are not explicitly supported by a given framework. This tutorial demonstrates how to run inference on topologies featuring custom layers allowing you to plug in your own implementation for existing or completely new layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Intel(R) Distribution of OpenVINO(TM) : Heterogenous Plugin\n",
    "\n",
    "At this session, we would like to go over more detailed explanation of OpenVINO(TM) Toolkit Inference Engine's Heterogeneous API which helps to run inference on heterogeneous platforms. \n",
    "\n",
    "If you are running on a platform with Intel CPU and GPU, you can control what layers of Deep Learning Model would be running on the selected hardware platform. \n",
    "\n",
    "Heterogeneous API has been developed first with fallback principle, it means whenever a deep learning layer not implemented for target platform it should fallback to CPU, which can execute all layers.  \n",
    "\n",
    "Not all layer's implementations are complete for each platform and certain layers can execute a lot faster on different platforms where some can't. As such, there has been great progress over the heterogeneous API use cases for Inference to analyze the performance bottlenecks and improve the inference process. \n",
    "\n",
    "At this section, we want to showcase its use on object detection scenario and get detailed analysis of executions of layers on devices a quick course of Heterogeneous Plugin of Intel(R) Distribution of OpenVINO(TM) Toolkit.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Load Libraries & Implement Helper Methods\n",
    "\n",
    "As in all previous sections, we again use `createNetwork`, `loadNetwork` and `prerocessImage` method before implementing inference with heterogeneous plugin.\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Import Required Libraries first\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# Import OpenVINO\n",
    "from openvino.inference_engine import IENetwork, IEPlugin\n",
    "\n",
    "# For labeling the image after inference.\n",
    "from utils.out_process import placeBoxes\n",
    "\n",
    "# Define Methods\n",
    "\n",
    "def createNetwork(model_xml, model_bin, plugin):\n",
    "    # Importing network weights from IR models.\n",
    "    net = IENetwork(model=model_xml, weights=model_bin)\n",
    "        \n",
    "    return net\n",
    "\n",
    "def loadNetwork(plugin, net, num_requests=2):\n",
    "    # Loading IR model to the plugin.\n",
    "    exec_net = plugin.load(network=net, num_requests=num_requests)\n",
    "    \n",
    "    # Getting the input and outputs of the network\n",
    "    input_blob = next(iter(net.inputs))\n",
    "    out_blob = next(iter(net.outputs))\n",
    "    \n",
    "    return exec_net,input_blob,out_blob\n",
    "\n",
    "def preprocessImage(img_path, net, input_blob):\n",
    "    # Reading the frame from a jpeg file\n",
    "    frame = cv.imread(img_path)\n",
    "    \n",
    "    # Reshaping data\n",
    "    n, c, h, w = net.inputs[input_blob].shape\n",
    "    in_frame = cv.resize(frame, (w, h))\n",
    "    in_frame = in_frame.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n",
    "    \n",
    "    return in_frame.reshape((n, c, h, w)),frame\n",
    "\n",
    "print('SUCCESS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this part, we are implementing a new inference method which is working with heterogeneous plugin. \n",
    "\n",
    "There are couple more steps we have added to inference process.\n",
    "\n",
    "- First we check if there is a CPU device on the list of devices, we load cpu extensions library.\n",
    "\n",
    "- Then, we load the network as usual process. \n",
    "\n",
    "-  `set_config` function configures the plugin fallback devices and their order.\n",
    "\n",
    "Heterogeneous Plugin used with `HETERO:` prefix, which is followed with a list of devices, according to their prioritization. \n",
    "\n",
    "- After network load, we send the network to plugin to set affinity of layers according to priority list we give to plugin. When affinity sets the layer's target device to be executed.\n",
    "\n",
    "- Additionally we have performance counters or certain outputs we can later analyze the network. `.set_config({\"HETERO_DUMP_GRAPH_DOT\": \"YES\"})` prints .dot output which has a detailed graph representation of model. \n",
    "\n",
    "- Finally, `get_perf_counts` method prints the detailed execution times of layers on the devices.\n",
    "\n",
    "Let's run the next cell and make `runInference` method ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Implement New `runInference` Method\n",
    "\n",
    "In this implementation we will do as defined in previous part. If you examine the code you will see the steps as following:\n",
    "\n",
    "- Load Plugin, set `device` parameter with 'HETERO:GPU,CPU` \n",
    "\n",
    "- Then parse plugin string to check if any extention is required. \n",
    "\n",
    "- Create network\n",
    "\n",
    "- Set Configurations: \n",
    "    - `TARGET_FALLBACK`\n",
    "    \n",
    "    - `HETERO_DUMP_GRAPH_DOT` to output graph output.\n",
    "    \n",
    "- Generate Executable Network and Run Inference\n",
    "\n",
    "- At the end of the script, if enabled prints `performance counters` values. \n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this stage we implement our inference method to run with Heterogeneous plugin. \n",
    "\n",
    "def runInference(hetero_device = 'HETERO:GPU,CPU',\n",
    "                 model_xml='/home/intel/openvino_models/models/object_detection/common/mobilenet-ssd/FP32/mobilenet-ssd.xml',\n",
    "                 model_bin='/home/intel/openvino_models/models/object_detection/common/mobilenet-ssd/FP32/mobilenet-ssd.bin',\n",
    "                 image_file='images/car.png',\n",
    "                 performance_counters = False,\n",
    "                 dot_graph = True,\n",
    "                 confidence_threshold=0.6):\n",
    "\n",
    "    # Plugin initialization for specified device. We will be targeting CPU initially.\n",
    "    plugin = IEPlugin(device=hetero_device, plugin_dirs='/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64')\n",
    "    \n",
    "    # Heterogeneous Plugin is provided as\n",
    "    #HETERO:GPU,CPU or similar.\n",
    "    vals = hetero_device.split(':')\n",
    "    targets = vals[1].split(',')\n",
    "    \n",
    "    # Let's check if heterogeneous plugin has CPU target so we can add cpu extensions\n",
    "    # Loading additional exension libraries for the CPU\n",
    "    if hetero_device == 'CPU' or ('CPU' in targets):\n",
    "        extension_list=['/home/intel/inference_engine_samples_build/intel64/Release/lib/libcpu_extension.so']\n",
    "        for extension in extension_list:\n",
    "            plugin.add_cpu_extension(extension)\n",
    " \n",
    "    net = createNetwork(model_xml, model_bin, plugin)\n",
    "    \n",
    "     # Let's check if the target is heterogeneous then we add fallback device priorities\n",
    "     #If we set TARGET_FALLBACK configuration, we want IE to decide what device to fallback. \n",
    "    \n",
    "    plugin.set_initial_affinity(net)\n",
    "    plugin.set_config({\"TARGET_FALLBACK\": hetero_device})\n",
    "    \n",
    "    if dot_graph:\n",
    "        # Just for a show case, remove\n",
    "        plugin.set_config({\"HETERO_DUMP_GRAPH_DOT\": \"YES\"})\n",
    "    \n",
    "    exec_net, input_blob, out_blob = loadNetwork(plugin, net)\n",
    "    \n",
    "    in_frame,original_frame = preprocessImage(image_file, net, input_blob)\n",
    "\n",
    "    my_request_id = 0\n",
    "\n",
    "        # Starting the inference in async mode, which starts the inference in parallel\n",
    "    inference_start = time.time()\n",
    "    exec_net.start_async(request_id=my_request_id, inputs={input_blob: in_frame})\n",
    "    # ... You can do additional processing or latency masking while we wait ...\n",
    "\n",
    "    # Blocking wait for a particular request_id\n",
    "    if exec_net.requests[my_request_id].wait(-1) == 0:\n",
    "        # getting the result of the network\n",
    "        res = exec_net.requests[my_request_id].outputs[out_blob]\n",
    "        inference_end = time.time()\n",
    "        # Processing the output result and adding labels on the image. Implementation is not shown in the\n",
    "        #  this notebook; you can find it in object_detection_demo_ssd_async.py\n",
    "        initial_w = original_frame.shape[1]\n",
    "        initial_h = original_frame.shape[0]\n",
    "\n",
    "        frame = placeBoxes(res, None, confidence_threshold, original_frame, initial_w, initial_h, False, my_request_id, ((inference_end - inference_start)))\n",
    "        # We use pyplot because it plays nicer with Jupyter Notebooks\n",
    "        fig = plt.figure(dpi=300)\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.imshow(cv.cvtColor(frame, cv.COLOR_BGR2RGB), interpolation='none')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"There was an error with the request\")\n",
    "    \n",
    "    if performance_counters:\n",
    "        perf_counts = exec_net.requests[0].get_perf_counts()\n",
    "        print(\"Performance counters:\")\n",
    "        for layer, stats in perf_counts.items():\n",
    "            print(layer, ': ', stats)\n",
    "    \n",
    "    return (plugin, net, exec_net)\n",
    "\n",
    "print('SUCCESS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In next cell, I would like to use Heterogeneous plugin to run object detection sample. \n",
    "\n",
    "With `HETERO:GPU,CPU,MYRIAD` we indicate to our plugin to prioritize `GPU > CPU > MYRIAD` for layers to be executed. \n",
    "\n",
    "For MobileNet-SSD example, almost all layers can run on GPU except `PriorBox` Caffe layer. Therefore, it will use GPU for all the `Convolution` layers and CPU for PriorBox layer.\n",
    "\n",
    "If, we have been used `HETERO:GPU,MYRIAD,CPU` , `PriorBox` layer would be running on MYRIAD since it has also support for it. \n",
    "\n",
    "Note that, we use FP16 because, in case CPU target is being used, IE helps to convert FP16 layers to FP32 automatically.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Run Inference\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code with : (SHIFT + ENTER) or Press Run Button #\n",
    "\n",
    "hetero_objects = runInference(hetero_device = 'HETERO:GPU,CPU',\n",
    "                              model_xml='/home/intel/openvino_models/models/object_detection/common/mobilenet-ssd/FP16/mobilenet-ssd.xml',\n",
    "                              model_bin='/home/intel/openvino_models/models/object_detection/common/mobilenet-ssd/FP16/mobilenet-ssd.bin',\n",
    "                              image_file='images/car.png',\n",
    "                              performance_counters = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running principles of Hetergeneous plugin is simple but if you want to take all the advantages of process and manually change the running layers and their target layers you should use layer affinity feature the change the execution target device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Layer Affinity\n",
    "\n",
    "Affinity by meaning likeness so you can image like a linkage of layer to target device. \n",
    "\n",
    "In this part we will look at how we can use this feature with Python API.\n",
    "\n",
    "This is a simple output of below code showing layer's affinity.\n",
    "\n",
    "```\n",
    "Type:  Input Device:  CPU\n",
    "Type:  ScaleShift Device:  GPU\n",
    "Type:  Convolution Device:  GPU\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Print Layer Affinities of Previous Section. \n",
    "\n",
    "Below part reads IE network object and prints the network layer values. \n",
    "\n",
    "**Note**: Please make sure you already run the previous part.\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = hetero_objects[1]\n",
    "\n",
    "for l in net.layers.values():\n",
    "    print('Type: ', l.type, 'Device: ', l.affinity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`{\"HETERO_DUMP_GRAPH_DOT\": \"YES\"}` configuraion let's us to print the network visualisation in .dot graph format. You can navigate to current directory and run it as below from a new terminal.\n",
    "\n",
    "\n",
    "```bash\n",
    "!xdot hetero_affinity_MobileNet-SSD.dot\n",
    "```\n",
    "\n",
    "![Dot Output](images/affinity_dot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Change Priority in Heterogeneous Plugin\n",
    "\n",
    "Let's use CPU First\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hetero_objects = runInference(hetero_device = 'HETERO:CPU,GPU',\n",
    "                              model_xml='/home/intel/openvino_models/models/object_detection/common/mobilenet-ssd/FP16/mobilenet-ssd.xml',\n",
    "                              model_bin='/home/intel/openvino_models/models/object_detection/common/mobilenet-ssd/FP16/mobilenet-ssd.bin',\n",
    "                              image_file='images/car.png',\n",
    "                              performance_counters = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Let's Check Layer Affinities \n",
    "\n",
    "You will notice that all layers assigned to CPU now.\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = hetero_objects[1]\n",
    "\n",
    "for l in net.layers.values():\n",
    "    print('Type: ', l.type, 'Device: ', l.affinity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Let's Try with Myriad \n",
    "\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hetero_objects = runInference(hetero_device = 'HETERO:MYRIAD,GPU,CPU',\n",
    "                              model_xml='/home/intel/openvino_models/models/object_detection/common/mobilenet-ssd/FP16/mobilenet-ssd.xml',\n",
    "                              model_bin='/home/intel/openvino_models/models/object_detection/common/mobilenet-ssd/FP16/mobilenet-ssd.bin',\n",
    "                              image_file='images/car.png',\n",
    "                              performance_counters = False)\n",
    "\n",
    "net = hetero_objects[1]\n",
    "\n",
    "for l in net.layers.values():\n",
    "    print('Type: ', l.type, 'Device: ', l.affinity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Performance Counters\n",
    "\n",
    "Inference Engine implements additional feature to let you know detailed performance report with performance counters. You can access them using Executable Network. \n",
    "\n",
    "Below code will report the layer name and its execution information. \n",
    "\n",
    "**NOTE**: Layet Affinity is only be used with Heterogeneous plugin since other plugins already determines the layer's execution target devices, performance counters can be used with all plugins.\n",
    "\n",
    "#### Change Focus to Below Cell and Press (SHIFT + ENTER) or Click on Run Button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the exec net\n",
    "exec_net = hetero_objects[2]\n",
    "\n",
    "# Print the performance counteres\n",
    "perf_counts = exec_net.requests[0].get_perf_counts()\n",
    "print(\"Performance counters:\")\n",
    "for layer, stats in perf_counts.items():\n",
    "    print(layer, ': ', stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hetero_objects = runInference(hetero_device = 'HETERO:CPU,GPU',\n",
    "                              model_xml='/home/intel/openvino_models/models/object_detection/common/mobilenet-ssd/FP16/mobilenet-ssd.xml',\n",
    "                              model_bin='/home/intel/openvino_models/models/object_detection/common/mobilenet-ssd/FP16/mobilenet-ssd.bin',\n",
    "                              image_file='images/car.png',\n",
    "                              performance_counters = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the exec net\n",
    "exec_net = hetero_objects[2]\n",
    "\n",
    "# Print the performance counteres\n",
    "perf_counts = exec_net.requests[0].get_perf_counts()\n",
    "print(\"Performance counters:\")\n",
    "for layer, stats in perf_counts.items():\n",
    "    print(layer, ': ', stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 - Custom Layers\n",
    "\n",
    "**NOTE**: This is a more advanced use case of OpenVINO(TM) Toolkit you may skip this part if you didn't cover the previous sections.\n",
    "\n",
    "This is a replication of https://github.com/intel-iot-devkit/smart-video-workshop/tree/master/custom-layer workshop.\n",
    "\n",
    "### Custom Layers\n",
    "\n",
    "Custom layers are NN (Neural Network) layers that are not explicitly supported by a given framework. This tutorial demonstrates how to run inference on topologies featuring custom layers allowing you to plug in your own implementation for existing or completely new layers.\n",
    "\n",
    "The list of known layers is different for any particular framework. To see the layers supported by the Intel® Distribution of OpenVINO™ toolkit, refer to the Documentation: https://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html#intermediate-representation-notation-catalog \n",
    "\n",
    "\n",
    "If your topology contains layers that are not in the list of known layers, the Model Optimizer considers them to be custom.\n",
    "\n",
    "The Model Optimizer searches for each layer of the input model in the list of known layers before building the model's internal representation, optimizing the model and producing the Intermediate Representation.\n",
    "\n",
    "Custom Layers implementation workflow in the Intel® Distribution of OpenVINO™ toolkit\n",
    "When implementing the custom layer in the Intel® Distribution of OpenVINO™ toolkit for your pre-trained model, you will need to add extensions in both the Model Optimizer and the Inference Engine. The following figure shows the work flow for the custom layer implementation. \n",
    "\n",
    "![MO Workflow](images/model_optimizer_workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example custom layer: Hyperbolic Cosine (cosh) function\n",
    "We showcase custom layer implementation using a simple function; hyperbolic cosine (cosh). It's mathematically represented as:\n",
    "\n",
    "![COSH](images/cosh_function.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension Generator\n",
    "This tool generates extension source files with stubs for the core functions. To get the workable extension, you will add your implementation of these functions to the generated files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Install Dependencies\n",
    "\n",
    "We need to execute all the commands through terminal so before going further, let's open a new terminal `CTRL + ALT + T`\n",
    "\n",
    "Let's first install the dependencies, copy the below code to terminal to setup `cogapp` package\n",
    "\n",
    "```bash\n",
    "sudo pip3 install cogapp\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Create TF Model\n",
    "\n",
    "Let's navigate to source folder where the resources are placed.\n",
    "\n",
    "```bash\n",
    "cd /home/intel/smart-video-workshop/custom-layer/create_tf_model\n",
    "```\n",
    "\n",
    "Then, let's build the model.\n",
    "\n",
    "We create a simple model with a custom cosh layer. The weights are random and untrained, however sufficient for demonstrating Custom Layer conversion.\n",
    "\n",
    "```bash\n",
    "mkdir -p tf_model\n",
    "\n",
    "chmod +x build_cosh_model.py\n",
    "\n",
    "./build_cosh_model.py tf_model\n",
    "```\n",
    "\n",
    "You will see that model file is generated into `tf_model` folder.\n",
    "\n",
    "```\n",
    "Model saved in path: tf_model/model.ckpt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Generate template files using the Extension Generator:\n",
    "We're using `/home/intel/smart-video-workshop/custom-layer/extgen_output/` as the target extension path:\n",
    "\n",
    "This will create templates that will be partially replaced by Python* and C++ code for executing the layer.\n",
    "\n",
    "```bash\n",
    "mkdir -p /home/intel/smart-video-workshop/custom-layer/extgen_output/\n",
    "\n",
    "python3 /opt/intel/openvino/deployment_tools/tools/extension_generator/extgen.py new --mo-tf-ext --mo-op --ie-cpu-ext --ie-gpu-ext --output_dir=/home/intel/smart-video-workshop/custom-layer/extgen_output/\n",
    "\n",
    "```\n",
    "\n",
    "Answer the Model Optimizer extension generator questions as follows:\n",
    "\n",
    "```bash\n",
    "Please enter layer name:\n",
    "[Cosh]\n",
    "\n",
    "Do you want to automatically parse all parameters from model file...\n",
    "[n]\n",
    "\n",
    "Please enter all parameters in format...\n",
    "When you finish please enter 'q'\n",
    "[q]\n",
    "\n",
    "Do you want to change any answer (y/n) ?\n",
    "[n]\n",
    "\n",
    "Please enter operation name:\n",
    "[Cosh]\n",
    "\n",
    "Please input all attributes that should be output in IR...\n",
    "...\n",
    "When you finish enter 'q'\n",
    "[q]\n",
    "\n",
    "Please input all internal attributes for your operation...\n",
    "...\n",
    "When you finish enter 'q'\n",
    "[q]\n",
    "\n",
    "Does your operation change shape? (y/n)\n",
    "[n]\n",
    "\n",
    "Do you want to change any answer (y/n) ?\n",
    "[n]\n",
    "\n",
    "Please enter operation name:\n",
    "[Cosh]\n",
    "\n",
    "Please enter all parameters in format...\n",
    "...\n",
    "When you finish please enter 'q'\n",
    "[q]\n",
    "\n",
    "Do you want to change any answer (y/n) ?\n",
    "[n]\n",
    "\n",
    "Please enter operation name:\n",
    "[Cosh]\n",
    "\n",
    "Please enter all parameters in format...\n",
    "...\n",
    "When you finish please enter 'q'\n",
    "[q]\n",
    "\n",
    "Do you want to change any answer (y/n) ?\n",
    "[n]\n",
    "\n",
    "```\n",
    "\n",
    "Output will be like below:\n",
    "\n",
    "```bash\n",
    "Stub file for TensorFlow Model Optimizer extractor is in /home/intel/smart-video-workshop/custom-layer/extgen_output/user_mo_extensions/front/tf folder\n",
    "Stub file for Model Optimizer operation is in /home/intel/smart-video-workshop/custom-layer/extgen_output/./user_mo_extensions/ops folder\n",
    "Stub files for Inference Engine CPU extension are in /home/intel/smart-video-workshop/custom-layer/extgen_output/./user_ie_extensions/cpu folder\n",
    "Stub files for Inference Engine GPU extension are in /home/intel/smart-video-workshop/custom-layer/extgen_output/./user_ie_extensions/gpu folder\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Register custom layer for the Model Optimizer\n",
    "\n",
    "Add Custom (cosh) Python Layers: Copy to the Model Optimizer Ops Directory:\n",
    "\n",
    "This allows the Model Optimizer to find the Python implementation of cosh.\n",
    "\n",
    "```bash\n",
    "sudo cp /home/intel/smart-video-workshop/custom-layer/cosh.py /opt/intel/openvino/deployment_tools/model_optimizer/mo/ops/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Generate IR with custom layer using Model Optimizer\n",
    "We run the Model Optimizer for TensorFlow to convert and optimize the new model for the Intel® Distribution of OpenVINO™ toolkit. We explicitly set the batch to 1 because the model has an input dim of \"-1\". TensorFlow allows \"-1\" as a variable indicating \"to be filled in later\", but the Model Optimizer requires explicit information for the optimization process. The output is the full name of the final output layer.\n",
    "\n",
    "```bash\n",
    "cd tf_model\n",
    "\n",
    "mkdir -p /home/intel/smart-video-workshop/custom-layer/cl_ext_cosh\n",
    "\n",
    "python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_tf.py --input_meta_graph model.ckpt.meta --batch 1 --output \"ModCosh/Activation_8/softmax_output\" --extensions /home/intel/smart-video-workshop/custom-layer/extgen_output/user_mo_extensions --output_dir /home/intel/smart-video-workshop/custom-layer/create_tf_model/tf_model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Inference Engine custom layer implementation for the Intel® CPU\n",
    "\n",
    "### Copy CPU and GPU source code to the Model Optimizer extensions directory:\n",
    "\n",
    "This will be used for building a back-end library for applications that implement cosh.\n",
    "\n",
    "```bash\n",
    "cp /home/intel/smart-video-workshop/custom-layer/ext_cosh.cpp /home/intel/smart-video-workshop/custom-layer/extgen_output/user_ie_extensions/cpu/\n",
    "```\n",
    "\n",
    "### Compile the C++ extension library:\n",
    "Here we're building the back-end C++ library to be used by the Inference Engine for executing the cosh layer.\n",
    "\n",
    "**NOTE**: Make sure you changed line in CMakeLists.txt \n",
    "\n",
    "***\n",
    "/opt/intel/openvino_2019.1.094/deployment_tools/inference_engine/external/tbb/include\n",
    "***\n",
    "\n",
    "to\n",
    "\n",
    "***\n",
    "/opt/intel/openvino/deployment_tools/inference_engine/external/tbb/include\n",
    "***\n",
    "\n",
    "```bash\n",
    "cd /home/intel/smart-video-workshop/custom-layer/extgen_output/user_ie_extensions/cpu\n",
    "\n",
    "cp /home/intel/smart-video-workshop/custom-layer/CMakeLists.txt .\n",
    "\n",
    "mkdir -p build && cd build\n",
    "\n",
    "cmake ..\n",
    "\n",
    "make -j$(nproc)\n",
    "\n",
    "cp libcosh_cpu_extension.so /home/intel/smart-video-workshop/custom-layer/cl_ext_cosh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Run Sample Code\n",
    "\n",
    "#### Using a C++ Sample:\n",
    "\n",
    "```bash\n",
    "/home/intel/inference_engine_samples_build/intel64/Release/classification_sample -i /opt/intel/openvino/deployment_tools/demo/car.png -m /home/intel/smart-video-workshop/custom-layer/create_tf_model/tf_model/model.ckpt.xml -d CPU -l /home/intel/smart-video-workshop/custom-layer/cl_ext_cosh/libcosh_cpu_extension.so\n",
    "```\n",
    "\n",
    "#### Using a Python Sample:\n",
    "\n",
    "Prep: Install the OpenCV library and copy an appropriate sample to your home directory for ease of use:\n",
    "\n",
    "Try running the Python Sample without including the cosh extension library. \n",
    "\n",
    "You should see the error describing unsupported Cosh operation.\n",
    "\n",
    "```bash\n",
    "python3 /opt/intel/openvino/deployment_tools/inference_engine/samples/python_samples/classification_sample/classification_sample.py -i /opt/intel/openvino/deployment_tools/demo/car.png  -m /home/intel/smart-video-workshop/custom-layer/create_tf_model/tf_model/model.ckpt.xml -d CPU\n",
    "```\n",
    "\n",
    "Error Output:\n",
    "***\n",
    "```\n",
    "[ INFO ] Loading network files:\n",
    "\t/home/intel/smart-video-workshop/custom-layer/create_tf_model/tf_model/model.ckpt.xml\n",
    "\t/home/intel/smart-video-workshop/custom-layer/create_tf_model/tf_model/model.ckpt.bin\n",
    "[ ERROR ] Following layers are not supported by the plugin for specified device CPU:\n",
    " ModCosh/cosh/Cosh, ModCosh/cosh_1/Cosh, ModCosh/cosh_2/Cosh\n",
    "[ ERROR ] Please try to specify cpu extensions library path in sample's command line parameters using -l or --cpu_extension command line argument\n",
    "```\n",
    "***\n",
    "\n",
    "- Now run the command with the cosh extension library:\n",
    "\n",
    "```\n",
    "python3 /opt/intel/openvino/deployment_tools/inference_engine/samples/python_samples/classification_sample/classification_sample.py -i /opt/intel/openvino/deployment_tools/demo/car.png  -m /home/intel/smart-video-workshop/custom-layer/create_tf_model/tf_model/model.ckpt.xml -l /home/intel/smart-video-workshop/custom-layer/cl_ext_cosh/libcosh_cpu_extension.so -d CPU\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
