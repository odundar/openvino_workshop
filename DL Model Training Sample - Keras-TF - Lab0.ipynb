{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model Training\n",
    "\n",
    "This is a simple Deep Learning Model Training Sample using Keras High Level API.\n",
    "\n",
    "Keras is a high-level neural networks API, written in Python which contains a set of helper methods and libraries to define neural networks. \n",
    "\n",
    "Keras also comes with a access to some certain set of open data set for training purposes and Keras also provides some utility methods for pre-processing training data. \n",
    "\n",
    "Keras is self is a high level API so it needs a backend, it can work on Tensorflow, CNTK and Theano.\n",
    "\n",
    "In following example, we will go over training a model with famous MNIST (hand-written digits) data set to create a DL model which can predict hand written digits. \n",
    "\n",
    "- Good visual ilustration of the model we will build\n",
    "http://scs.ryerson.ca/~aharley/vis/conv/\n",
    "\n",
    "First, we start importing required libraries to start with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Database - Handwritten digits (0-9)\n",
    "\n",
    "On this tutorial we will use Python* to implement one Convolutional Neural Network - a simplified version of LeNet - that will recognized Handwritten digits. A project like this one, using the MNIST dataset is considered as the \"Hello World\" of Machine Learning.\n",
    "\n",
    "We will use Keras*, TensorFlow* and the MNIST database.\n",
    "\n",
    "According to the description on their website, \"Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\"*\n",
    "\n",
    "We will use TensorFlow as the backend for Keras. TensorFlow is an open source software library for high performance numerical computation.\n",
    "\n",
    "The MNIST database is a large database of handwritten digits that is commonly used for training various image processing systems. MNIST database is also available as a Keras dataset, with 60k 28x28 images of the 10 digits along with a test set of 10k images, so it is very easy to import and use it on our code.\n",
    "\n",
    "One good visual and interactive reference on what we are developing can be found here. The basic difference between our code and this interactive sample is the number and size of convolutional and fully-connected layers (LeNet uses two of each, we will use a single one, to reduce training time). We also adjusted the layers size to balance between accuracy and training time. We are achieving 98,54% of accuracy with less than 2 minutes training time on an Intel® Core™ processor.\n",
    "\n",
    "This code can also be optimized by several ways to increase accuracy, and we would like to invite you to explore this later, changing the number of epochs, filters, fully-connected neurons and also including additional convolutional and fully connected layers. You can also use flattening, dropout and batch normalization layers. Other optimization techniques can also be applied, so feel free to use this tutorial code as a base to explore those optimization techniques.\n",
    "\n",
    "In a nutshell, the convolutional and pooling layers are responsible for extracting a set of features from the input images, and the fully-connected layers are responsible for classification.\n",
    "\n",
    "Convolutional layers applies a set of filters to the input image to extract important features from the image. The filters are small matrixes also called image kernels that can be repeatedly applied to the input image (\"sliding\" the filter on the image). You may already used those filters on traditional image processing applications such as GIMP (i.e. blurring, sharpening or embossing). This article gives a good overview on image kernels with some live experiments. Each filter will generate a new image that will be the input for the next layer, typically a pooling layer.\n",
    "\n",
    "Pooling layers reduces the spatial size of the image (downsampling), reducing the computation in the network and also controlling overfitting.\n",
    "\n",
    "Fully connected layers are traditional Neural Network layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Sequential Network Model https://keras.io/models/sequential/\n",
    "from keras.models import Sequential\n",
    "# Core Layers https://keras.io/layers/core/\n",
    "# Dense: densely-connected NN layer, to be used as classification layer\n",
    "# Flatten: layer to flatten the convolutional layers\n",
    "from keras.layers import Dense, Flatten\n",
    "# Convolutional Layers https://keras.io/layers/convolutional/\n",
    "# Conv2D: 2D convolution Layer\n",
    "from keras.layers import Conv2D\n",
    "# Pooling Layer: https://keras.io/layers/pooling/\n",
    "# MaxPooling2D: Max pooling operation for spatial data\n",
    "from keras.layers import MaxPooling2D\n",
    "# Utilities https://keras.io/utils/\n",
    "from keras.utils import np_utils\n",
    "# MNIST Dataset https://keras.io/datasets/\n",
    "# Dataset of 60,000 28x28 handwritten images of the 10 digits, along with a test set of 10,000 images.\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, we will first load the dataset. using mnist interface. Then, we go with pre-processing data set to make it ready to be accepted in Input layer for DL.\n",
    "\n",
    "Then, we make sure type is float, DL models only uses floating point values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 8s 1us/step\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST data set in two sets: Trainning (60K IMAGES) and Testing (10k images)\n",
    "(train_dataset, train_classes),(test_dataset, test_classes) = mnist.load_data()\n",
    "\n",
    "# Adjust datasets to TensorFlow\n",
    "# Reduce image channels from 3 to 1\n",
    "train_dataset = train_dataset.reshape(train_dataset.shape[0], 28, 28, 1)\n",
    "test_dataset = test_dataset.reshape(test_dataset.shape[0], 28, 28, 1)\n",
    "\n",
    "# Covert data from int8 to float32\n",
    "train_dataset = train_dataset.astype('float32')\n",
    "test_dataset = test_dataset.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are doing normalization of the dataset. DL models, can work on normalized data a lot faster and accurate. heterogenous numbers tend to over-fit and hard to converge during training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data to speed up processing time\n",
    "train_dataset = train_dataset / 255\n",
    "test_dataset = test_dataset / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this part, we convert output to a categorical class representation. During the training process, our model will try to converge certain values, mainly 0 or 1. However in this case we have 10 different category. Therefore, we create a tensor with 1D shape where training value only can be one of the values.\n",
    "\n",
    "- e.g. label of training input is 2.\n",
    "- DL output/class is [0., 0., 1., ....]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class data from numerical to categorical\n",
    "train_classes = np_utils.to_categorical(train_classes, 10)\n",
    "test_classes = np_utils.to_categorical(test_classes, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the final part where we create a basic Convolutional Neural Network with using Keras Layers. \n",
    "\n",
    "Below is a very basic CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5408)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               692352    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 693,962\n",
      "Trainable params: 693,962\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the Convolutional Neural Network\n",
    "cnn = Sequential()\n",
    "\n",
    "# Add the convolutional layer with 32 filters, 3x3 convolution window,\n",
    "# 28 x 28 x 1 pixels imput array and Rectified Linear Unit activation function\n",
    "cnn.add(Conv2D(32, (3,3), input_shape = (28, 28, 1), activation = 'relu'))\n",
    "\n",
    "# Add one Pooling layer with default 2x2 size\n",
    "cnn.add(MaxPooling2D())\n",
    "\n",
    "# Add one flattening layer to convert the output matrix to a vector to be the Deep Neural Network input\n",
    "cnn.add(Flatten())\n",
    "\n",
    "# Add one hidden layer with 128 neurons and Rectified Linear Unit activation function\n",
    "cnn.add(Dense(units = 128, activation = 'relu'))\n",
    "\n",
    "# Add the output layer with 10 neurons (one for each class) with Softmax as the activation function\n",
    "cnn.add(Dense(units = 10, activation = 'softmax'))\n",
    "\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the last part, where loss function, optimizer and metrics has been defined for training process. \n",
    "\n",
    "`.fit` method used to start training process with provided training and label data.\n",
    "\n",
    "When, fit finished your model is ready to predict. \n",
    "\n",
    "`.evaluate` used to check results with test data set and see the accuracy of your model with a data set never seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 23s 376us/step - loss: 0.2121 - acc: 0.9393 - val_loss: 0.0818 - val_acc: 0.9747\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 21s 350us/step - loss: 0.0641 - acc: 0.9806 - val_loss: 0.0622 - val_acc: 0.9790\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 22s 361us/step - loss: 0.0450 - acc: 0.9865 - val_loss: 0.0461 - val_acc: 0.9841\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 22s 360us/step - loss: 0.0321 - acc: 0.9902 - val_loss: 0.0431 - val_acc: 0.9846\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 21s 349us/step - loss: 0.0249 - acc: 0.9928 - val_loss: 0.0381 - val_acc: 0.9877\n",
      "10000/10000 [==============================] - 1s 141us/step\n",
      "Accuracy = 98.77%\n"
     ]
    }
   ],
   "source": [
    "# Compile the CNN with:\n",
    "#  - Categorical crossentropy as the loss function\n",
    "#  - Adam optimizer\n",
    "#  - Accuracy as the results evaluation metric\n",
    "cnn.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "# Execute the training on 5 epochs, validating the generated model with test dataset on each epoch\n",
    "cnn.fit(train_dataset, train_classes, batch_size = 128, epochs = 5, validation_data = (test_dataset, test_classes))\n",
    "\n",
    "# Extract and print the Accuracy results\n",
    "result = cnn.evaluate(test_dataset, test_classes)\n",
    "print ('Accuracy = ' + str(result[1] * 100) + \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
