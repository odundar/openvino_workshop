{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intel(R) Media SDK Utilisation in Video Applications\n",
    "\n",
    "In previous labs, we have focuced on DL inference on the video applications but a lot more going on a video analytics applications. Intel provides a lot more tool to enhace the process.\n",
    "\n",
    "At this section, we will run 6 application developed with OpenCV, Inference Engine and Media SDK C++ APIs each named as `tutorial_0` to `tutorial_5`.\n",
    "\n",
    "All the source code of these examples placed under `/home/intel/Tutorials/interop_tutorials` folder.\n",
    "\n",
    "|App Name   | Decoding   | Pre-Process | Inference   | Post-Process | Encoding  |\n",
    "|-----------|------------|-------------|-------------|--------------|-----------|\n",
    "|Tutorial 0 | OpenCV     |     OpenCV  | OpenCV      | OpenCV       | OpenCV    |\n",
    "|Tutorial 1 | OpenCV     |     OpenCV  | OpenVINO    | OpenCV       | OpenCV    |\n",
    "|Tutorial 2 | Media SDK  |     OpenCV  | OpenVINO    | OpenCV       | OpenCV    |\n",
    "|Tutorial 3 | Media SDK  |  Media SDK  | OpenVINO    | OpenCV       | OpenCV    |\n",
    "|Tutorial 4 | Media SDK  |  Media SDK  | OpenVINO    | Media SDK    | OpenCV    |\n",
    "|Tutorial 5 | Media SDK  |  Media SDK  | OpenVINO    | Media SDK    | Media SDK |\n",
    "\n",
    "In this set of tutorails, we will investigate how Intel(R) Media SDK into an End to End Video Application.\n",
    "\n",
    "All applications uses `SSD GoogleNet v2` for object recognition this time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1\n",
    "\n",
    "Tutorial 1 does all decoding, pre and post-processing part with OpenCV* and inference with Intel(R) Distribution of OpenVINO(TM).\n",
    "\n",
    "Let's try out Tutorial 1 sample application.\n",
    "\n",
    "Open a new terminal. (ctrl+alt+t)\n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "export LD_LIBRARY_PATH=/home/intel/inference_engine_samples/intel64/Release/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "cd /home/intel/Tutorials/interop_tutorials/tutorial_1\n",
    "\n",
    "./tutorial1 -h\n",
    "\n",
    "[usage]\n",
    "\ttutorial_1 [option]\n",
    "\toptions:\n",
    "\n",
    "\t\t-h                  Print a usage message\n",
    "\t\t-i <path/filename>  Required. Path to input video file\n",
    "\t\t-fr <val>           Number of frames from stream to process\n",
    "\t\t-m <path/filename>  Required. Path to IR .xml file.\n",
    "\t\t-l <path/filename>  Required. Path to labels file.\n",
    "\t\t-d <device>         Infer target device (CPU or GPU or MYRIAD)\n",
    "\t\t-pc                 Enables per-layer performance report\n",
    "\t\t-thresh <val>       Confidence threshold for bounding boxes 0-1\n",
    "\t\t-batch <val>        Batch size\n",
    "\t\t-s                  Display less information on the screen\n",
    "\t\t-e <path/filename>  Load layer extension plugin\n",
    "\t\t-mean               Mean value for normalization of data during planar BGR blob preprocess step\n",
    "\t\t-scale              Scale value for normalization of data during planar BGR blob preprocess step\n",
    "\t\t-show               Display a window for composited image with ROI\n",
    "\n",
    "```\n",
    "\n",
    "As seen from above example, this application is able to load OpenVINO model and is able to run inference on CPU, GPU and MYRIAD. \n",
    "\n",
    "We will use `run.sh` script to run `tutorial1` on CPU and `run_gpu.sh` to run inference on GPU. You can change commands in scripts as you wish. \n",
    "\n",
    "```bash\n",
    "\n",
    "./run.sh\n",
    "\n",
    "> Pre-stage average:\t10.00 ms/frame (decoding, color converting, resizing)\n",
    "> Infer average:\t55.19 ms/frame (inferencing)\n",
    "> Post-stage average:\t0.65 ms/frame (drawing bounding box, encoding, saving)\n",
    "\n",
    "> Total elapsed execution time: 17.12 sec\n",
    "\n",
    "Done!\n",
    "```\n",
    "\n",
    "Let's run on GPU, this time IE loads clDNNPlugin. \n",
    "\n",
    "```bash\n",
    "\n",
    "./run_gpu.sh\n",
    "\n",
    "> Pre-stage average:\t10.41 ms/frame (decoding, color converting, resizing)\n",
    "> Infer average:\t27.67 ms/frame (inferencing)\n",
    "> Post-stage average:\t0.71 ms/frame (drawing bounding box, encoding, saving)\n",
    "\n",
    "> Total elapsed execution time: 10.01 sec\n",
    "\n",
    "Done!\n",
    "\n",
    "```\n",
    "\n",
    "Final output of inference written to out.h264 file, you can play it with mplayer and see the inference output.\n",
    "\n",
    "```bash\n",
    "mplayer out.h264`\n",
    "```\n",
    "\n",
    "Let's get into details and investigate the perofmance little more.\n",
    "\n",
    "## Performance Analysis\n",
    "\n",
    "As we seen from the previous sections. We are able to get performance analysis. It is also implemented for this application. It will be a messy output to see each inference performance outputs.\n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "export LD_LIBRARY_PATH=/home/intel/inference_engine_samples/intel64/Release/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "/home/intel/Tutorials/interop_tutorials/tutorial_1/tutorial1 -m /home/intel/Tutorials/test_content/IR/SSD/SSD_GoogleNet_v2_fp16.xml -i /home/intel/Tutorials/test_content/video/cars_1920x1080.h264 -l /home/intel/Tutorials/test_content/IR/SSD/pascal_voc_classes.txt -d GPU -pc\n",
    "```\n",
    "\n",
    "## Output Video\n",
    "\n",
    "Let's see the output interactively with `-show` option.\n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "export LD_LIBRARY_PATH=/home/intel/inference_engine_samples/intel64/Release/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "/home/intel/Tutorials/interop_tutorials/tutorial_1/tutorial1 -m /home/intel/Tutorials/test_content/IR/SSD/SSD_GoogleNet_v2_fp16.xml -i /home/intel/Tutorials/test_content/video/cars_1920x1080.h264 -l /home/intel/Tutorials/test_content/IR/SSD/pascal_voc_classes.txt -d GPU -show\n",
    "```\n",
    "\n",
    "## Batch Support\n",
    "\n",
    "Let's see how we can use a larger batch size with this example.\n",
    "\n",
    "At the output, you should realize that, it does show 32 inferences which means it packed 256 frames to batches and does inference at the all 8 same time, and we gain a lot of time, total execution decreased a lot. \n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "export LD_LIBRARY_PATH=/home/intel/inference_engine_samples/intel64/Release/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "/home/intel/Tutorials/interop_tutorials/tutorial_1/tutorial1 -m /home/intel/Tutorials/test_content/IR/SSD/SSD_GoogleNet_v2_fp16.xml -i /home/intel/Tutorials/test_content/video/cars_1920x1080.h264 -l /home/intel/Tutorials/test_content/IR/SSD/pascal_voc_classes.txt -d GPU -batch 8\n",
    "\n",
    "Batch: 29/32\n",
    "\tpre-stage:\t5.84 ms/frame\n",
    "\tinfer:\t\t21.98 ms/frame\n",
    "\tpost-stage:\t5.39 ms/frame\n",
    "\n",
    "Batch: 30/32\n",
    "\tpre-stage:\t6.43 ms/frame\n",
    "\tinfer:\t\t22.82 ms/frame\n",
    "\tpost-stage:\t5.38 ms/frame\n",
    "\n",
    "Batch: 31/32\n",
    "\tpre-stage:\t5.99 ms/frame\n",
    "\tinfer:\t\t22.39 ms/frame\n",
    "\tpost-stage:\t4.69 ms/frame\n",
    "\n",
    "\n",
    "\n",
    "> Pre-stage average:\t8.98 ms/frame (decoding, color converting, resizing)\n",
    "> Infer average:\t22.18 ms/frame (inferencing)\n",
    "> Post-stage average:\t5.64 ms/frame (drawing bounding box, encoding, saving)\n",
    "\n",
    "> Total elapsed execution time: 8.62 sec\n",
    "\n",
    "Done!\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 2\n",
    "\n",
    "In Tutorial 2 application, we add Media SDK for decoding only, and Media SDK uses system memory at this stage. In this tutorial and the followings, we don't have display but they all saving the output of the video. \n",
    "\n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "export LD_LIBRARY_PATH=/home/intel/inference_engine_samples/intel64/Release/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "cd /home/intel/Tutorials/interop_tutorials/tutorial_2\n",
    "\n",
    "./tutorial2 -h\n",
    "\n",
    "[usage]\n",
    "\ttutorial_2 [option]\n",
    "\toptions:\n",
    "\n",
    "\t\t-h                  Print a usage message\n",
    "\t\t-i <path/filename>  Required. Path to input video file, video elementary stream only\n",
    "\t\t-fr <val>           Number of frames from stream to process\n",
    "\t\t-m <path/filename>  Required. Path to IR .xml file.\n",
    "\t\t-l <path/filename>  Required. Path to labels file.\n",
    "\t\t-d <device>         Infer target device (CPU or GPU or MYRIAD)\n",
    "\t\t-pc                 Enables per-layer performance report\n",
    "\t\t-thresh <val>       Confidence threshold for bounding boxes 0-1\n",
    "\t\t-batch <val>        Batch size\n",
    "\t\t-s                  Display less information on the screen\n",
    "\t\t-e <path/filename>  Load layer extension plugin\n",
    "\t\t-mean               Mean value for normalization of data during planar BGR blob preprocess step\n",
    "\t\t-scale              Scale value for normalization of data during planar BGR blob preprocess step\n",
    "\n",
    "```\n",
    "\n",
    "As seen from above example, this application is able to load OpenVINO model and is able to run inference on CPU, GPU and MYRIAD. \n",
    "\n",
    "We will use run.sh file to run tutorial2 on CPU and run_gpu.sh to run inference on GPU.\n",
    "\n",
    "We see that there is a particular improvement on inference. \n",
    "\n",
    "```bash\n",
    "\n",
    "./run.sh\n",
    "\n",
    "> Pre-stage average:\t11.92 ms/frame (decoding, color converting, resizing)\n",
    "> Infer average:\t55.72 ms/frame (inferencing)\n",
    "> Post-stage average:\t0.50 ms/frame (drawing bounding box, encoding, saving)\n",
    "\n",
    "> Total elapsed execution time: 17.62 sec\n",
    "\n",
    "> Done ! (Output is in out.h264 -> $ mplayer out.h264)\n",
    "\n",
    "```\n",
    "\n",
    "Let's run on GPU, this time IE loads clDNNPlugin. Now, you will realized that there isn't any improvement on the performance. Why? \n",
    "\n",
    "```bash\n",
    "\n",
    "./run_gpu.sh\n",
    "\n",
    "> Pre-stage average:\t10.93 ms/frame (decoding, color converting, resizing)\n",
    "> Infer average:\t40.24 ms/frame (inferencing)\n",
    "> Post-stage average:\t0.62 ms/frame (drawing bounding box, encoding, saving)\n",
    "\n",
    "> Total elapsed execution time: 13.32 sec\n",
    "\n",
    "> Done ! (Output is in out.h264 -> $ mplayer out.h264)\n",
    "\n",
    "```\n",
    "\n",
    "Final output of inference written to out.h264 file, you can play it with mplayer and see the inference output.\n",
    "\n",
    "```bash\n",
    "mplayer out.h264`\n",
    "```\n",
    "\n",
    "Let's get into details and investigate the perofmance little more.\n",
    "\n",
    "## Performance Analysis\n",
    "\n",
    "As we seen from the previous sections. We are able to get performance analysis. It is also implemented for this application. It will be a messy output to see each inference performance outputs.\n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "export LD_LIBRARY_PATH=/home/intel/inference_engine_samples/intel64/Release/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "/home/intel/Tutorials/interop_tutorials/tutorial_2/tutorial2 -m /home/intel/Tutorials/test_content/IR/SSD/SSD_GoogleNet_v2_fp16.xml -i /home/intel/Tutorials/test_content/video/cars_1920x1080.h264 -l /home/intel/Tutorials/test_content/IR/SSD/pascal_voc_classes.txt -d GPU -pc\n",
    "```\n",
    "\n",
    "## Batch Support\n",
    "\n",
    "Let's see how we can use a larger batch size with this example.\n",
    "\n",
    "At the output, you should realize that, it does show 32 inferences which means it packed 256 frames to batches and does inference at the all 8 same time. \n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "export LD_LIBRARY_PATH=/home/intel/inference_engine_samples/intel64/Release/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "/home/intel/Tutorials/interop_tutorials/tutorial_2/tutorial2 -m /home/intel/Tutorials/test_content/IR/SSD/SSD_GoogleNet_v2_fp16.xml -i /home/intel/Tutorials/test_content/video/cars_1920x1080.h264 -l /home/intel/Tutorials/test_content/IR/SSD/pascal_voc_classes.txt -d GPU -batch 8\n",
    "\n",
    "Batch: 28/32\n",
    "\tpre-stage:\t6.44 ms/frame\n",
    "\tinfer:\t\t22.91 ms/frame\n",
    "\tpost-stage:\t1.61 ms/frame\n",
    "\n",
    "Batch: 29/32\n",
    "\tpre-stage:\t6.78 ms/frame\n",
    "\tinfer:\t\t22.73 ms/frame\n",
    "\tpost-stage:\t2.14 ms/frame\n",
    "\n",
    "Batch: 30/32\n",
    "\tpre-stage:\t6.56 ms/frame\n",
    "\tinfer:\t\t22.39 ms/frame\n",
    "\tpost-stage:\t1.61 ms/frame\n",
    "\n",
    "\n",
    "\n",
    "> Pre-stage average:\t10.44 ms/frame (decoding, color converting, resizing)\n",
    "> Infer average:\t22.62 ms/frame (inferencing)\n",
    "> Post-stage average:\t2.13 ms/frame (drawing bounding box, encoding, saving)\n",
    "\n",
    "> Total elapsed execution time: 9.20 sec\n",
    "\n",
    "> Done ! (Output is in out.h264 -> $ mplayer out.h264)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3\n",
    "\n",
    "In Tutorial 3 application, we add Media SDK for decoding and pre-processing let's see difference on the pre-stage.\n",
    "\n",
    "Let's open a new terminal. \n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "export LD_LIBRARY_PATH=/home/intel/inference_engine_samples/intel64/Release/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "cd /home/intel/Tutorials/interop_tutorials/tutorial_3\n",
    "\n",
    "./tutorial3 -h\n",
    "\n",
    "[usage]\n",
    "\ttutorial_3 [option]\n",
    "\toptions:\n",
    "\n",
    "\t\t-h                  Print a usage message\n",
    "\t\t-i <path/filename>  Required. Path to input video file, video elementary stream only\n",
    "\t\t-fr <val>           Number of frames from stream to process\n",
    "\t\t-m <path/filename>  Required. Path to IR .xml file.\n",
    "\t\t-l <path/filename>  Required. Path to labels file.\n",
    "\t\t-d <device>         Infer target device (CPU or GPU or MYRIAD)\n",
    "\t\t-pc                 Enables per-layer performance report\n",
    "\t\t-thresh <val>       Confidence threshold for bounding boxes 0-1\n",
    "\t\t-batch <val>            Batch size\n",
    "\t\t-s                  Display less information on the screen\n",
    "\t\t-e <path/filename>  Load layer extension plugin\n",
    "\t\t-mean               Mean value for normalization of data during planar BGR blob preprocess step\n",
    "\t\t-scale              Scale value for normalization of data during planar BGR blob preprocess step\n",
    "\n",
    "```\n",
    "\n",
    "As seen from above example, this application is able to load OpenVINO model and is able to run inference on CPU, GPU and MYRIAD. \n",
    "\n",
    "We will use run.sh file to run tutorial3 on CPU and run_gpu.sh to run inference on GPU.\n",
    "\n",
    "We see that there is a particular improvement on inference. \n",
    "\n",
    "```bash\n",
    "\n",
    "./run.sh\n",
    "\n",
    "> Pre-stage average:\t7.97 ms/frame (decoding, color converting, resizing)\n",
    "> Infer average:\t55.23 ms/frame (inferencing)\n",
    "> Post-stage average:\t0.56 ms/frame (drawing bounding box, encoding, saving)\n",
    "\n",
    "> Total elapsed execution time: 16.45 sec\n",
    "\n",
    "> Done ! (Output is in out.h264 -> $ mplayer out.h264)\n",
    "\n",
    "```\n",
    "\n",
    "Let's run on GPU, this time IE loads clDNNPlugin. \n",
    "\n",
    "```bash\n",
    "\n",
    "./run_gpu.sh\n",
    "\n",
    "> Pre-stage average:\t8.89 ms/frame (decoding, color converting, resizing)\n",
    "> Infer average:\t40.29 ms/frame (inferencing)\n",
    "> Post-stage average:\t0.75 ms/frame (drawing bounding box, encoding, saving)\n",
    "\n",
    "> Total elapsed execution time: 12.85 sec\n",
    "\n",
    "> Done ! (Output is in out.h264 -> $ mplayer out.h264)\n",
    "\n",
    "```\n",
    "\n",
    "Final output of inference written to out.h264 file, you can play it with mplayer and see the inference output.\n",
    "\n",
    "```bash\n",
    "mplayer out.h264\n",
    "```\n",
    "\n",
    "Let's get into details and investigate the perofmance little more.\n",
    "\n",
    "## Performance Analysis\n",
    "\n",
    "As we seen from the previous sections. We are able to get performance analysis. It is also implemented for this application. It will be a messy output to see each inference performance outputs.\n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "export LD_LIBRARY_PATH=/home/intel/inference_engine_samples/intel64/Release/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "/home/intel/Tutorials/interop_tutorials/tutorial_3/tutorial3 -m /home/intel/Tutorials/test_content/IR/SSD/SSD_GoogleNet_v2_fp16.xml -i /home/intel/Tutorials/test_content/video/cars_1920x1080.h264 -l /home/intel/Tutorials/test_content/IR/SSD/pascal_voc_classes.txt -d GPU -pc\n",
    "```\n",
    "\n",
    "## Batch Support\n",
    "\n",
    "Let's see how we can use a larger batch size with this example.\n",
    "\n",
    "At the output, you should realize that, it does show 32 inferences which means it packed 256 frames to batches and does inference at the all 8 same time. \n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "export LD_LIBRARY_PATH=/home/intel/inference_engine_samples/intel64/Release/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "/home/intel/Tutorials/interop_tutorials/tutorial_3/tutorial3 -m /home/intel/Tutorials/test_content/IR/SSD/SSD_GoogleNet_v2_fp16.xml -i /home/intel/Tutorials/test_content/video/cars_1920x1080.h264 -l /home/intel/Tutorials/test_content/IR/SSD/pascal_voc_classes.txt -d GPU -batch 8\n",
    "\n",
    "Batch: 30/32\n",
    "\tpre-stage:\t8.21 ms/frame\n",
    "\tinfer:\t\t21.79 ms/frame\n",
    "\tpost-stage:\t2.25 ms/frame\n",
    "\n",
    "Batch: 31/32\n",
    "\tpre-stage:\t9.61 ms/frame\n",
    "\tinfer:\t\t22.20 ms/frame\n",
    "\tpost-stage:\t2.32 ms/frame\n",
    "\n",
    "Batch: 32/32\n",
    "\tpre-stage:\t8.91 ms/frame\n",
    "\tinfer:\t\t22.20 ms/frame\n",
    "\tpost-stage:\t3.41 ms/frame\n",
    "\n",
    "> Pre-stage average:\t8.33 ms/frame (decoding, color converting, resizing)\n",
    "> Infer average:\t22.43 ms/frame (inferencing)\n",
    "> Post-stage average:\t2.43 ms/frame (drawing bounding box, encoding, saving)\n",
    "\n",
    "> Total elapsed execution time: 8.54 sec\n",
    "\n",
    "> Done ! (Output is in out.h264 -> $ mplayer out.h264)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4\n",
    "\n",
    "In Tutorial 4 application, we add Media SDK for decoding, pre-processing and post-processing steps let's see difference on the pre-stage. Difference on Tutorial 4 is that, Media SDK uses Video Memory instead of System Memory.\n",
    "\n",
    "Let's open a new terminal. \n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "export LD_LIBRARY_PATH=/home/intel/inference_engine_samples/intel64/Release/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "cd /home/intel/Tutorials/interop_tutorials/tutorial_4\n",
    "\n",
    "./tutorial4 -h\n",
    "\n",
    "[usage]\n",
    "\ttutorial_4 [option]\n",
    "\toptions:\n",
    "\n",
    "\t\t-h                  Print a usage message\n",
    "\t\t-i <path/filename>  Required. Path to input video file, video elementary stream only\n",
    "\t\t-fr <val>           Number of frames from stream to process\n",
    "\t\t-m <path/filename>  Required. Path to IR .xml file.\n",
    "\t\t-l <path/filename>  Required. Path to labels file.\n",
    "\t\t-d <device>         Infer target device (CPU or GPU or MYRIAD)\n",
    "\t\t-pc                 Enables per-layer performance report\n",
    "\t\t-thresh <val>       Confidence threshold for bounding boxes 0-1\n",
    "\t\t-batch <val>        Batch size\n",
    "\t\t-s                  Display less information on the screen\n",
    "\t\t-e <path/filename>  Load layer extension plugin\n",
    "\t\t-mean               Mean value for normalization of data during planar BGR blob preprocess step\n",
    "\t\t-scale              Scale value for normalization of data during planar BGR blob preprocess step\n",
    "\n",
    "```\n",
    "\n",
    "As seen from above example, this application is able to load OpenVINO model and is able to run inference on CPU, GPU and MYRIAD. \n",
    "\n",
    "We will use run.sh file to run tutorial4 on CPU and run_gpu.sh to run inference on GPU.\n",
    "\n",
    "We see that there is a particular improvement on inference. \n",
    "\n",
    "```bash\n",
    "\n",
    "./run.sh\n",
    "\n",
    "> Pre-stage average:\t5.01 ms/frame (decoding, color converting, resizing)\n",
    "> Infer average:\t55.29 ms/frame (inferencing)\n",
    "> Post-stage average:\t0.52 ms/frame (drawing bounding box, encoding, saving)\n",
    "\n",
    "> Total elapsed execution time: 15.71 sec\n",
    "\n",
    "> Done ! (Output is in out.h264 -> $ mplayer out.h264)\n",
    "\n",
    "```\n",
    "\n",
    "Let's run on GPU, this time IE loads clDNNPlugin. \n",
    "\n",
    "```bash\n",
    "\n",
    "./run_gpu.sh\n",
    "\n",
    "> Pre-stage average:\t4.95 ms/frame (decoding, color converting, resizing)\n",
    "> Infer average:\t42.14 ms/frame (inferencing)\n",
    "> Post-stage average:\t0.54 ms/frame (drawing bounding box, encoding, saving)\n",
    "\n",
    "> Total elapsed execution time: 12.28 sec\n",
    "\n",
    "> Done ! (Output is in out.h264 -> $ mplayer out.h264)\n",
    "\n",
    "```\n",
    "\n",
    "Final output of inference written to out.h264 file, you can play it with mplayer and see the inference output.\n",
    "\n",
    "```bash\n",
    "mplayer out.h264\n",
    "```\n",
    "\n",
    "Let's get into details and investigate the perofmance little more.\n",
    "\n",
    "## Performance Analysis\n",
    "\n",
    "As we seen from the previous sections. We are able to get performance analysis. It is also implemented for this application. It will be a messy output to see each inference performance outputs.\n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "export LD_LIBRARY_PATH=/home/intel/inference_engine_samples/intel64/Release/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "/home/intel/Tutorials/interop_tutorials/tutorial_4/tutorial4 -m /home/intel/Tutorials/test_content/IR/SSD/SSD_GoogleNet_v2_fp16.xml -i /home/intel/Tutorials/test_content/video/cars_1920x1080.h264 -l /home/intel/Tutorials/test_content/IR/SSD/pascal_voc_classes.txt -d GPU -pc\n",
    "```\n",
    "\n",
    "## Batch Support\n",
    "\n",
    "Let's see how we can use a larger batch size with this example.\n",
    "\n",
    "At the output, you should realize that, it does show 32 inferences which means it packed 256 frames to batches and does inference at the all 8 same time. \n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "export LD_LIBRARY_PATH=/home/intel/inference_engine_samples/intel64/Release/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "/home/intel/Tutorials/interop_tutorials/tutorial_4/tutorial4 -m /home/intel/Tutorials/test_content/IR/SSD/SSD_GoogleNet_v2_fp16.xml -i /home/intel/Tutorials/test_content/video/cars_1920x1080.h264 -l /home/intel/Tutorials/test_content/IR/SSD/pascal_voc_classes.txt -d GPU -batch 8\n",
    "\n",
    "Batch: 30/32\n",
    "\tpre-stage:\t4.87 ms/frame\n",
    "\tinfer:\t\t22.76 ms/frame\n",
    "\tpost-stage:\t2.34 ms/frame\n",
    "\n",
    "Batch: 31/32\n",
    "\tpre-stage:\t4.74 ms/frame\n",
    "\tinfer:\t\t22.19 ms/frame\n",
    "\tpost-stage:\t2.23 ms/frame\n",
    "\n",
    "Batch: 32/32\n",
    "\tpre-stage:\t5.00 ms/frame\n",
    "\tinfer:\t\t22.05 ms/frame\n",
    "\tpost-stage:\t2.28 ms/frame\n",
    "\n",
    "\n",
    "> Pre-stage average:\t4.97 ms/frame (decoding, color converting, resizing)\n",
    "> Infer average:\t22.03 ms/frame (inferencing)\n",
    "> Post-stage average:\t1.91 ms/frame (drawing bounding box, encoding, saving)\n",
    "\n",
    "> Total elapsed execution time: 7.51 sec\n",
    "\n",
    "> Done ! (Output is in out.h264 -> $ mplayer out.h264)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5\n",
    "\n",
    "In Tutorial 5 application, we add Media SDK for decoding, pre-processing, post-processing and encoding steps let's see difference on the pre-stage.\n",
    "\n",
    "Let's open a new terminal. \n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "export LD_LIBRARY_PATH=/home/intel/inference_engine_samples/intel64/Release/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "cd /home/intel/Tutorials/interop_tutorials/tutorial_5\n",
    "\n",
    "./tutorial5 -h\n",
    "\n",
    "[usage]\n",
    "\ttutorial_5 [option]\n",
    "\toptions:\n",
    "\n",
    "\t\t-h                  Print a usage message\n",
    "\t\t-i <path/filename>  Required. Path to input video file, video elementary stream only\n",
    "\t\t-fr <val>           Number of frames from stream to process\n",
    "\t\t-m <path/filename>  Required. Path to IR .xml file.\n",
    "\t\t-l <path/filename>  Required. Path to labels file.\n",
    "\t\t-d <device>         Infer target device (CPU or GPU or MYRIAD)\n",
    "\t\t-pc                 Enables per-layer performance report\n",
    "\t\t-thresh <val>       Confidence threshold for bounding boxes 0-1\n",
    "\t\t-b <val>            Batch size\n",
    "\t\t-s                  Display less information on the screen\n",
    "\t\t-e <path/filename>  Load layer extension plugin\n",
    "\t\t-mean               Mean value for normalization of data during planar BGR blob preprocess step\n",
    "\t\t-scale              Scale value for normalization of data during planar BGR blob preprocess step\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "As seen from above example, this application is able to load OpenVINO model and is able to run inference on CPU, GPU and MYRIAD. \n",
    "\n",
    "Let's run on GPU, this time IE loads clDNNPlugin. \n",
    "\n",
    "```bash\n",
    "\n",
    "./run.sh\n",
    "\n",
    "> Pre-stage average:\t4.82 ms/frame (decoding, color converting, resizing)\n",
    "> Infer average:\t55.69 ms/frame (inferencing)\n",
    "> Post-stage average:\t2.61 ms/frame (drawing bounding box, encoding, saving)\n",
    "\n",
    "> Total elapsed execution time: 16.24 sec\n",
    "\n",
    "```\n",
    "\n",
    "```bash\n",
    "\n",
    "./run_gpu.sh\n",
    "\n",
    "\n",
    "> Pre-stage average:\t4.89 ms/frame (decoding, color converting, resizing)\n",
    "> Infer average:\t40.73 ms/frame (inferencing)\n",
    "> Post-stage average:\t2.75 ms/frame (drawing bounding box, encoding, saving)\n",
    "\n",
    "> Total elapsed execution time: 12.43 sec\n",
    "\n",
    "Done!\n",
    "\n",
    "```\n",
    "\n",
    "Final output of inference written to out.h264 file, you can play it with mplayer and see the inference output.\n",
    "\n",
    "```bash\n",
    "mplayer out.h264\n",
    "```\n",
    "\n",
    "Let's get into details and investigate the perofmance little more.\n",
    "\n",
    "## Performance Analysis\n",
    "\n",
    "As we seen from the previous sections. We are able to get performance analysis. It is also implemented for this application. It will be a messy output to see each inference performance outputs.\n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "export LD_LIBRARY_PATH=/home/intel/inference_engine_samples/intel64/Release/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "/home/intel/Tutorials/interop_tutorials/tutorial_5/tutorial5 -m /home/intel/Tutorials/test_content/IR/SSD/SSD_GoogleNet_v2_fp16.xml -i /home/intel/Tutorials/test_content/video/cars_1920x1080.h264 -l /home/intel/Tutorials/test_content/IR/SSD/pascal_voc_classes.txt -d GPU -pc\n",
    "```\n",
    "\n",
    "## Batch Support\n",
    "\n",
    "Let's see how we can use a larger batch size with this example.\n",
    "\n",
    "At the output, you should realize that, it does show 32 inferences which means it packed 256 frames to batches and does inference at the all 8 same time. \n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "export LD_LIBRARY_PATH=/home/intel/inference_engine_samples/intel64/Release/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "/home/intel/Tutorials/interop_tutorials/tutorial_5/tutorial5 -m /home/intel/Tutorials/test_content/IR/SSD/SSD_GoogleNet_v2_fp16.xml -i /home/intel/Tutorials/test_content/video/cars_1920x1080.h264 -l /home/intel/Tutorials/test_content/IR/SSD/pascal_voc_classes.txt -d GPU -batch 8\n",
    "\n",
    "Batch: 31/32\n",
    "\tpre-stage:\t7.95 ms/frame\n",
    "\tinfer:\t\t22.76 ms/frame\n",
    "\tpost-stage:\t5.50 ms/frame\n",
    "\n",
    "Batch: 32/32\n",
    "\tpre-stage:\t8.23 ms/frame\n",
    "\tinfer:\t\t22.83 ms/frame\n",
    "\tpost-stage:\t5.95 ms/frame\n",
    "\n",
    "Batch: 33/32\n",
    "\tpre-stage:\t6.54 ms/frame\n",
    "    \n",
    "\n",
    "> Pre-stage average:\t5.58 ms/frame (decoding, color converting, resizing)\n",
    "> Infer average:\t22.83 ms/frame (inferencing)\n",
    "> Post-stage average:\t7.73 ms/frame (drawing bounding box, encoding, saving)\n",
    "\n",
    "> Total elapsed execution time: 9.32 sec\n",
    "\n",
    "\n",
    "Done!\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
