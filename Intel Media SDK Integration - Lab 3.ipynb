{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intel Media SDK Utilisation in Video Applications\n",
    "\n",
    "In previous labs, we have focuced on DL inference on the video applications but a lot more going on a video analytics applications. Intel provides a lot more tool to enhace the process.\n",
    "\n",
    "At this section, we will run 6 application developed with OpenCV, Inference Engine and Media SDK C++ APIs each named as `tutorial_0` to `tutorial_5`.\n",
    "\n",
    "All the source code of these examples placed under `/home/intel/Tutorials/interop_tutorials` folder.\n",
    "\n",
    "\n",
    "|App Name   | Decoding   | Pre-Process | Inference   | Post-Process | Encoding  |\n",
    "|-----------|------------|-------------|-------------|--------------|-----------|\n",
    "|Tutorial 0 | OpenCV     |     OpenCV  | OpenCV      | OpenCV       | OpenCV    |\n",
    "|Tutorial 1 | OpenCV     |     OpenCV  | OpenVINO    | OpenCV       | OpenCV    |\n",
    "|Tutorial 2 | Media SDK  |     OpenCV  | OpenVINO    | OpenCV       | OpenCV    |\n",
    "|Tutorial 3 | Media SDK  |  Media SDK  | OpenVINO    | OpenCV       | OpenCV    |\n",
    "|Tutorial 4 | Media SDK  |  Media SDK  | OpenVINO    | Media SDK    | OpenCV    |\n",
    "|Tutorial 5 | Media SDK  |  Media SDK  | OpenVINO    | Media SDK    | Media SDK |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 0\n",
    "\n",
    "Tutorial 0 is all implemented with OpenCV package of Intel(R) Distribution of OpenVINO. \n",
    "\n",
    "We want to show the legacy performance metrics with Tutorial 0 at start.\n",
    "\n",
    "Let's open a new terminal. \n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "\n",
    "cd /home/intel/Tutorials/interop_tutorials/tutorial_0\n",
    "\n",
    "./tutorial0 -h\n",
    "\n",
    "[usage]\n",
    "\ttutorial_0 [option]\n",
    "\toptions:\n",
    "\n",
    "\t\t-h              Print a usage message\n",
    "\t\t-i <path>       Required. Path to input video file\n",
    "\t\t-fr <path>      Number of frames from stream to process\n",
    "\t\t-m <path>       Required. Path to Caffe deploy.prototxt file.\n",
    "\t\t-weights <path> Required. Path to Caffe weights in .caffemodel file.\n",
    "\t\t-l <path>       Required. Path to labels file.\n",
    "\t\t-thresh <val>   Confidence threshold for bounding boxes 0-1\n",
    "\t\t-s              Display less information on the screen\n",
    "```\n",
    "\n",
    "As seen from above example, this application is able to load Caffe model and run inference on it. \n",
    "\n",
    "There is also a run.sh which runs the sample application with provided video input and reports out the preprocess, inference and post-process results as below.\n",
    "\n",
    "```bash\n",
    "\n",
    "./run.sh\n",
    "\n",
    "Batch: 255/256\n",
    "\tpre-stage:\t7.08 ms/frame\n",
    "\tinfer:\t\t87.22 ms/frame\n",
    "\tpost-stage:\t0.48 ms/frame\n",
    "\n",
    "Batch: 256/256\n",
    "\tpre-stage:\t7.05 ms/frame\n",
    "\tinfer:\t\t86.85 ms/frame\n",
    "\tpost-stage:\t0.49 ms/frame\n",
    "\n",
    "Batch: 257/256\n",
    "\tpre-stage:\t6.92 ms/frame\n",
    "\n",
    "\n",
    "> Pre-stage average:\t6.69 ms/frame (decoding, color converting, resizing)\n",
    "> Infer average:\t87.82 ms/frame (inferencing)\n",
    "> Post-stage average:\t0.52 ms/frame (drawing bounding box, encoding, saving)\n",
    "\n",
    "> Total elapsed execution time: 24.36 sec\n",
    "\n",
    "Done!\n",
    "\n",
    "```\n",
    "\n",
    "Here you see, that inference takes ~86 ms.\n",
    "\n",
    "You can see the output on the current directory you run as out.h264. Play the output file and see the inferences.\n",
    "\n",
    "```bash\n",
    "ls \n",
    "mplayer out.264\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1\n",
    "\n",
    "Tutorial 1 only replaces the inference part with Intel(R) Distribution of OpenVINO Inference Engine.\n",
    "\n",
    "\n",
    "Let's open a new terminal. \n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "export LD_LIBRARY_PATH=/home/intel/inference_engine_samples/intel64/Release/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "cd /home/intel/Tutorials/interop_tutorials/tutorial_1\n",
    "\n",
    "./tutorial1 -h\n",
    "\n",
    "[usage]\n",
    "\ttutorial_1 [option]\n",
    "\toptions:\n",
    "\n",
    "\t\t-h                  Print a usage message\n",
    "\t\t-i <path/filename>  Required. Path to input video file\n",
    "\t\t-fr <val>           Number of frames from stream to process\n",
    "\t\t-m <path/filename>  Required. Path to IR .xml file.\n",
    "\t\t-l <path/filename>  Required. Path to labels file.\n",
    "\t\t-d <device>         Infer target device (CPU or GPU or MYRIAD)\n",
    "\t\t-pc                 Enables per-layer performance report\n",
    "\t\t-thresh <val>       Confidence threshold for bounding boxes 0-1\n",
    "\t\t-batch <val>        Batch size\n",
    "\t\t-s                  Display less information on the screen\n",
    "\t\t-e <path/filename>  Load layer extension plugin\n",
    "\t\t-mean               Mean value for normalization of data during planar BGR blob preprocess step\n",
    "\t\t-scale              Scale value for normalization of data during planar BGR blob preprocess step\n",
    "\t\t-show               Display a window for composited image with ROI\n",
    "\n",
    "```\n",
    "\n",
    "As seen from above example, this application is able to load OpenVINO model and is able to run inference on CPU, GPU and MYRIAD. \n",
    "\n",
    "We will use run.sh file to run tutorial1 on CPU and run_gpu.sh to run inference on GPU.\n",
    "\n",
    "We see that there is a particular improvement on inference. \n",
    "\n",
    "```bash\n",
    "\n",
    "./run.sh\n",
    "\n",
    "Batch: 255/256\n",
    "\tpre-stage:\t10.52 ms/frame\n",
    "\tinfer:\t\t56.12 ms/frame\n",
    "\tpost-stage:\t0.45 ms/frame\n",
    "\n",
    "Batch: 256/256\n",
    "\tpre-stage:\t15.02 ms/frame\n",
    "\tinfer:\t\t52.21 ms/frame\n",
    "\tpost-stage:\t0.43 ms/frame\n",
    "\n",
    "Batch: 257/256\n",
    "\tpre-stage:\t9.70 ms/frame\n",
    "\n",
    "\n",
    "> Pre-stage average:\t10.41 ms/frame (decoding, color converting, resizing)\n",
    "> Infer average:\t54.44 ms/frame (inferencing)\n",
    "> Post-stage average:\t0.53 ms/frame (drawing bounding box, encoding, saving)\n",
    "\n",
    "> Total elapsed execution time: 16.81 sec\n",
    "\n",
    "Done!\n",
    "```\n",
    "\n",
    "Let's run on GPU, this time IE loads clDNNPlugin. \n",
    "\n",
    "```bash\n",
    "\n",
    "./run_gpu.sh\n",
    "\n",
    "Batch: 255/256\n",
    "\tpre-stage:\t11.44 ms/frame\n",
    "\tinfer:\t\t28.59 ms/frame\n",
    "\tpost-stage:\t0.49 ms/frame\n",
    "\n",
    "Batch: 256/256\n",
    "\tpre-stage:\t7.80 ms/frame\n",
    "\tinfer:\t\t27.85 ms/frame\n",
    "\tpost-stage:\t0.52 ms/frame\n",
    "\n",
    "Batch: 257/256\n",
    "\tpre-stage:\t11.04 ms/frame\n",
    "\n",
    "\n",
    "> Pre-stage average:\t10.16 ms/frame (decoding, color converting, resizing)\n",
    "> Infer average:\t27.67 ms/frame (inferencing)\n",
    "> Post-stage average:\t0.66 ms/frame (drawing bounding box, encoding, saving)\n",
    "\n",
    "> Total elapsed execution time: 9.93 sec\n",
    "\n",
    "Done!\n",
    "\n",
    "```\n",
    "\n",
    "Final output of inference written to out.h264 file, you can play it with mplayer and see the inference output.\n",
    "\n",
    "```bash\n",
    "mplayer out.h264`\n",
    "```\n",
    "\n",
    "Let's get into details and investigate the perofmance little more.\n",
    "\n",
    "## Performance Analysis\n",
    "\n",
    "As we seen from the previous sections. We are able to get performance analysis. It is also implemented for this application. It will be a messy output to see each inference performance outputs.\n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "export LD_LIBRARY_PATH=/home/intel/inference_engine_samples/intel64/Release/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "/home/intel/Tutorials/interop_tutorials/tutorial_1/tutorial1 -m /home/intel/Tutorials/test_content/IR/SSD/SSD_GoogleNet_v2_fp16.xml -i /home/intel/Tutorials/test_content/video/cars_768x768.h264 -l /home/intel/Tutorials/test_content/IR/SSD/pascal_voc_classes.txt -d GPU -pc\n",
    "```\n",
    "\n",
    "\n",
    "## Output Video\n",
    "\n",
    "Let's see the output interactively with `-show` option.\n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "export LD_LIBRARY_PATH=/home/intel/inference_engine_samples/intel64/Release/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "/home/intel/Tutorials/interop_tutorials/tutorial_1/tutorial1 -m /home/intel/Tutorials/test_content/IR/SSD/SSD_GoogleNet_v2_fp16.xml -i /home/intel/Tutorials/test_content/video/cars_768x768.h264 -l /home/intel/Tutorials/test_content/IR/SSD/pascal_voc_classes.txt -d GPU -show\n",
    "```\n",
    "\n",
    "## Batch Support\n",
    "\n",
    "Let's see how we can use a larger batch size with this example.\n",
    "\n",
    "At the output, you should realize that, it does show 32 inferences which means it packed 256 frames to batches and does inference at the all 8 same time. \n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "export LD_LIBRARY_PATH=/home/intel/inference_engine_samples/intel64/Release/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "/home/intel/Tutorials/interop_tutorials/tutorial_1/tutorial1 -m /home/intel/Tutorials/test_content/IR/SSD/SSD_GoogleNet_v2_fp16.xml -i /home/intel/Tutorials/test_content/video/cars_768x768.h264 -l /home/intel/Tutorials/test_content/IR/SSD/pascal_voc_classes.txt -d GPU -batch 8\n",
    "\n",
    "Batch: 29/32\n",
    "\tpre-stage:\t5.84 ms/frame\n",
    "\tinfer:\t\t21.98 ms/frame\n",
    "\tpost-stage:\t5.39 ms/frame\n",
    "\n",
    "Batch: 30/32\n",
    "\tpre-stage:\t6.43 ms/frame\n",
    "\tinfer:\t\t22.82 ms/frame\n",
    "\tpost-stage:\t5.38 ms/frame\n",
    "\n",
    "Batch: 31/32\n",
    "\tpre-stage:\t5.99 ms/frame\n",
    "\tinfer:\t\t22.39 ms/frame\n",
    "\tpost-stage:\t4.69 ms/frame\n",
    "\n",
    "\n",
    "\n",
    "> Pre-stage average:\t5.65 ms/frame (decoding, color converting, resizing)\n",
    "> Infer average:\t22.30 ms/frame (inferencing)\n",
    "> Post-stage average:\t5.31 ms/frame (drawing bounding box, encoding, saving)\n",
    "\n",
    "> Total elapsed execution time: 7.49 sec\n",
    "\n",
    "Done!\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 2\n",
    "\n",
    "In Tutorial 2 application, we add Media SDK for decoding only and see the difference on the pre-stage.\n",
    "\n",
    "Let's open a new terminal. \n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "export LD_LIBRARY_PATH=/home/intel/inference_engine_samples/intel64/Release/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "cd /home/intel/Tutorials/interop_tutorials/tutorial_2\n",
    "\n",
    "./tutorial2 -h\n",
    "\n",
    "[usage]\n",
    "\ttutorial_2 [option]\n",
    "\toptions:\n",
    "\n",
    "\t\t-h                  Print a usage message\n",
    "\t\t-i <path/filename>  Required. Path to input video file, video elementary stream only\n",
    "\t\t-fr <val>           Number of frames from stream to process\n",
    "\t\t-m <path/filename>  Required. Path to IR .xml file.\n",
    "\t\t-l <path/filename>  Required. Path to labels file.\n",
    "\t\t-d <device>         Infer target device (CPU or GPU or MYRIAD)\n",
    "\t\t-pc                 Enables per-layer performance report\n",
    "\t\t-thresh <val>       Confidence threshold for bounding boxes 0-1\n",
    "\t\t-batch <val>        Batch size\n",
    "\t\t-s                  Display less information on the screen\n",
    "\t\t-e <path/filename>  Load layer extension plugin\n",
    "\t\t-mean               Mean value for normalization of data during planar BGR blob preprocess step\n",
    "\t\t-scale              Scale value for normalization of data during planar BGR blob preprocess step\n",
    "\n",
    "```\n",
    "\n",
    "As seen from above example, this application is able to load OpenVINO model and is able to run inference on CPU, GPU and MYRIAD. \n",
    "\n",
    "We will use run.sh file to run tutorial2 on CPU and run_gpu.sh to run inference on GPU.\n",
    "\n",
    "We see that there is a particular improvement on inference. \n",
    "\n",
    "```bash\n",
    "\n",
    "./run.sh\n",
    "\n",
    "Batch: 254/256\n",
    "\tpre-stage:\t9.02 ms/frame\n",
    "\tinfer:\t\t53.26 ms/frame\n",
    "\tpost-stage:\t0.51 ms/frame\n",
    "\n",
    "Batch: 255/256\n",
    "\tpre-stage:\t11.34 ms/frame\n",
    "\tinfer:\t\t54.30 ms/frame\n",
    "\tpost-stage:\t0.42 ms/frame\n",
    "\n",
    "Batch: 256/256\n",
    "\tpre-stage:\t11.53 ms/frame\n",
    "\tinfer:\t\t54.07 ms/frame\n",
    "\tpost-stage:\t0.41 ms/frame\n",
    "\n",
    "\n",
    "\n",
    "> Pre-stage average:\t11.65 ms/frame (decoding, color converting, resizing)\n",
    "> Infer average:\t55.35 ms/frame (inferencing)\n",
    "> Post-stage average:\t0.52 ms/frame (drawing bounding box, encoding, saving)\n",
    "\n",
    "> Total elapsed execution time: 17.40 sec\n",
    "\n",
    "> Done ! (Output is in out.h264 -> $ mplayer out.h264)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Let's run on GPU, this time IE loads clDNNPlugin. \n",
    "\n",
    "```bash\n",
    "\n",
    "./run_gpu.sh\n",
    "\n",
    "Batch: 254/256\n",
    "\tpre-stage:\t9.37 ms/frame\n",
    "\tinfer:\t\t39.96 ms/frame\n",
    "\tpost-stage:\t0.49 ms/frame\n",
    "\n",
    "Batch: 255/256\n",
    "\tpre-stage:\t11.23 ms/frame\n",
    "\tinfer:\t\t39.23 ms/frame\n",
    "\tpost-stage:\t0.53 ms/frame\n",
    "\n",
    "Batch: 256/256\n",
    "\tpre-stage:\t11.12 ms/frame\n",
    "\tinfer:\t\t40.44 ms/frame\n",
    "\tpost-stage:\t0.76 ms/frame\n",
    "\n",
    "\n",
    "\n",
    "> Pre-stage average:\t11.13 ms/frame (decoding, color converting, resizing)\n",
    "> Infer average:\t40.62 ms/frame (inferencing)\n",
    "> Post-stage average:\t0.60 ms/frame (drawing bounding box, encoding, saving)\n",
    "\n",
    "> Total elapsed execution time: 13.50 sec\n",
    "\n",
    "> Done ! (Output is in out.h264 -> $ mplayer out.h264)\n",
    "```\n",
    "\n",
    "Final output of inference written to out.h264 file, you can play it with mplayer and see the inference output.\n",
    "\n",
    "```bash\n",
    "mplayer out.h264`\n",
    "```\n",
    "\n",
    "Let's get into details and investigate the perofmance little more.\n",
    "\n",
    "## Performance Analysis\n",
    "\n",
    "As we seen from the previous sections. We are able to get performance analysis. It is also implemented for this application. It will be a messy output to see each inference performance outputs.\n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "export LD_LIBRARY_PATH=/home/intel/inference_engine_samples/intel64/Release/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "/home/intel/Tutorials/interop_tutorials/tutorial_2/tutorial2 -m /home/intel/Tutorials/test_content/IR/SSD/SSD_GoogleNet_v2_fp16.xml -i /home/intel/Tutorials/test_content/video/cars_768x768.h264 -l /home/intel/Tutorials/test_content/IR/SSD/pascal_voc_classes.txt -d GPU -pc\n",
    "```\n",
    "\n",
    "## Batch Support\n",
    "\n",
    "Let's see how we can use a larger batch size with this example.\n",
    "\n",
    "At the output, you should realize that, it does show 32 inferences which means it packed 256 frames to batches and does inference at the all 8 same time. \n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "export LD_LIBRARY_PATH=/home/intel/inference_engine_samples/intel64/Release/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "/home/intel/Tutorials/interop_tutorials/tutorial_2/tutorial2 -m /home/intel/Tutorials/test_content/IR/SSD/SSD_GoogleNet_v2_fp16.xml -i /home/intel/Tutorials/test_content/video/cars_768x768.h264 -l /home/intel/Tutorials/test_content/IR/SSD/pascal_voc_classes.txt -d GPU -batch 8\n",
    "\n",
    "Batch: 28/32\n",
    "\tpre-stage:\t6.44 ms/frame\n",
    "\tinfer:\t\t22.91 ms/frame\n",
    "\tpost-stage:\t1.61 ms/frame\n",
    "\n",
    "Batch: 29/32\n",
    "\tpre-stage:\t6.78 ms/frame\n",
    "\tinfer:\t\t22.73 ms/frame\n",
    "\tpost-stage:\t2.14 ms/frame\n",
    "\n",
    "Batch: 30/32\n",
    "\tpre-stage:\t6.56 ms/frame\n",
    "\tinfer:\t\t22.39 ms/frame\n",
    "\tpost-stage:\t1.61 ms/frame\n",
    "\n",
    "\n",
    "\n",
    "> Pre-stage average:\t6.76 ms/frame (decoding, color converting, resizing)\n",
    "> Infer average:\t22.60 ms/frame (inferencing)\n",
    "> Post-stage average:\t1.85 ms/frame (drawing bounding box, encoding, saving)\n",
    "\n",
    "> Total elapsed execution time: 7.57 sec\n",
    "\n",
    "> Done ! (Output is in out.h264 -> $ mplayer out.h264)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3\n",
    "\n",
    "In Tutorial 3 application, we add Media SDK for decoding and pre-processing let's see difference on the pre-stage.\n",
    "\n",
    "Let's open a new terminal. \n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "export LD_LIBRARY_PATH=/home/intel/inference_engine_samples/intel64/Release/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "cd /home/intel/Tutorials/interop_tutorials/tutorial_3\n",
    "\n",
    "./tutorial3 -h\n",
    "\n",
    "[usage]\n",
    "\ttutorial_3 [option]\n",
    "\toptions:\n",
    "\n",
    "\t\t-h                  Print a usage message\n",
    "\t\t-i <path/filename>  Required. Path to input video file, video elementary stream only\n",
    "\t\t-fr <val>           Number of frames from stream to process\n",
    "\t\t-m <path/filename>  Required. Path to IR .xml file.\n",
    "\t\t-l <path/filename>  Required. Path to labels file.\n",
    "\t\t-d <device>         Infer target device (CPU or GPU or MYRIAD)\n",
    "\t\t-pc                 Enables per-layer performance report\n",
    "\t\t-thresh <val>       Confidence threshold for bounding boxes 0-1\n",
    "\t\t-batch <val>            Batch size\n",
    "\t\t-s                  Display less information on the screen\n",
    "\t\t-e <path/filename>  Load layer extension plugin\n",
    "\t\t-mean               Mean value for normalization of data during planar BGR blob preprocess step\n",
    "\t\t-scale              Scale value for normalization of data during planar BGR blob preprocess step\n",
    "\n",
    "```\n",
    "\n",
    "As seen from above example, this application is able to load OpenVINO model and is able to run inference on CPU, GPU and MYRIAD. \n",
    "\n",
    "We will use run.sh file to run tutorial3 on CPU and run_gpu.sh to run inference on GPU.\n",
    "\n",
    "We see that there is a particular improvement on inference. \n",
    "\n",
    "```bash\n",
    "\n",
    "./run.sh\n",
    "\n",
    "Batch: 254/256\n",
    "\tpre-stage:\t8.09 ms/frame\n",
    "\tinfer:\t\t53.41 ms/frame\n",
    "\tpost-stage:\t0.45 ms/frame\n",
    "\n",
    "Batch: 255/256\n",
    "\tpre-stage:\t7.31 ms/frame\n",
    "\tinfer:\t\t56.35 ms/frame\n",
    "\tpost-stage:\t2.44 ms/frame\n",
    "\n",
    "Batch: 256/256\n",
    "\tpre-stage:\t10.92 ms/frame\n",
    "\tinfer:\t\t63.68 ms/frame\n",
    "\tpost-stage:\t0.44 ms/frame\n",
    "\n",
    "\n",
    "\n",
    "> Pre-stage average:\t7.97 ms/frame (decoding, color converting, resizing)\n",
    "> Infer average:\t55.22 ms/frame (inferencing)\n",
    "> Post-stage average:\t0.57 ms/frame (drawing bounding box, encoding, saving)\n",
    "\n",
    "> Total elapsed execution time: 16.39 sec\n",
    "\n",
    "> Done ! (Output is in out.h264 -> $ mplayer out.h264)\n",
    "\n",
    "```\n",
    "\n",
    "Let's run on GPU, this time IE loads clDNNPlugin. \n",
    "\n",
    "```bash\n",
    "\n",
    "./run_gpu.sh\n",
    "\n",
    "Batch: 254/256\n",
    "\tpre-stage:\t7.67 ms/frame\n",
    "\tinfer:\t\t40.82 ms/frame\n",
    "\tpost-stage:\t0.49 ms/frame\n",
    "\n",
    "Batch: 255/256\n",
    "\tpre-stage:\t7.72 ms/frame\n",
    "\tinfer:\t\t38.60 ms/frame\n",
    "\tpost-stage:\t0.52 ms/frame\n",
    "\n",
    "Batch: 256/256\n",
    "\tpre-stage:\t10.43 ms/frame\n",
    "\tinfer:\t\t39.46 ms/frame\n",
    "\tpost-stage:\t0.58 ms/frame\n",
    "\n",
    "\n",
    "\n",
    "> Pre-stage average:\t8.92 ms/frame (decoding, color converting, resizing)\n",
    "> Infer average:\t39.93 ms/frame (inferencing)\n",
    "> Post-stage average:\t0.76 ms/frame (drawing bounding box, encoding, saving)\n",
    "\n",
    "> Total elapsed execution time: 12.77 sec\n",
    "\n",
    "> Done ! (Output is in out.h264 -> $ mplayer out.h264)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Final output of inference written to out.h264 file, you can play it with mplayer and see the inference output.\n",
    "\n",
    "```bash\n",
    "mplayer out.h264\n",
    "```\n",
    "\n",
    "Let's get into details and investigate the perofmance little more.\n",
    "\n",
    "## Performance Analysis\n",
    "\n",
    "As we seen from the previous sections. We are able to get performance analysis. It is also implemented for this application. It will be a messy output to see each inference performance outputs.\n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "export LD_LIBRARY_PATH=/home/intel/inference_engine_samples/intel64/Release/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "/home/intel/Tutorials/interop_tutorials/tutorial_3/tutorial3 -m /home/intel/Tutorials/test_content/IR/SSD/SSD_GoogleNet_v2_fp16.xml -i /home/intel/Tutorials/test_content/video/cars_1920x1080.h264 -l /home/intel/Tutorials/test_content/IR/SSD/pascal_voc_classes.txt -d GPU -pc\n",
    "```\n",
    "\n",
    "## Batch Support\n",
    "\n",
    "Let's see how we can use a larger batch size with this example.\n",
    "\n",
    "At the output, you should realize that, it does show 32 inferences which means it packed 256 frames to batches and does inference at the all 8 same time. \n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "export LD_LIBRARY_PATH=/home/intel/inference_engine_samples/intel64/Release/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "/home/intel/Tutorials/interop_tutorials/tutorial_3/tutorial3 -m /home/intel/Tutorials/test_content/IR/SSD/SSD_GoogleNet_v2_fp16.xml -i /home/intel/Tutorials/test_content/video/cars_1920x1080.h264 -l /home/intel/Tutorials/test_content/IR/SSD/pascal_voc_classes.txt -d GPU -batch 8\n",
    "\n",
    "Batch: 30/32\n",
    "\tpre-stage:\t8.21 ms/frame\n",
    "\tinfer:\t\t21.79 ms/frame\n",
    "\tpost-stage:\t2.25 ms/frame\n",
    "\n",
    "Batch: 31/32\n",
    "\tpre-stage:\t9.61 ms/frame\n",
    "\tinfer:\t\t22.20 ms/frame\n",
    "\tpost-stage:\t2.32 ms/frame\n",
    "\n",
    "Batch: 32/32\n",
    "\tpre-stage:\t8.91 ms/frame\n",
    "\tinfer:\t\t22.20 ms/frame\n",
    "\tpost-stage:\t3.41 ms/frame\n",
    "\n",
    "\n",
    "\n",
    "> Pre-stage average:\t8.46 ms/frame (decoding, color converting, resizing)\n",
    "> Infer average:\t22.43 ms/frame (inferencing)\n",
    "> Post-stage average:\t2.33 ms/frame (drawing bounding box, encoding, saving)\n",
    "\n",
    "> Total elapsed execution time: 8.53 sec\n",
    "\n",
    "> Done ! (Output is in out.h264 -> $ mplayer out.h264)\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4\n",
    "\n",
    "In Tutorial 4 application, we add Media SDK for decoding, pre-processing and post-processing steps let's see difference on the pre-stage.\n",
    "\n",
    "Let's open a new terminal. \n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "export LD_LIBRARY_PATH=/home/intel/inference_engine_samples/intel64/Release/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "cd /home/intel/Tutorials/interop_tutorials/tutorial_4\n",
    "\n",
    "./tutorial4 -h\n",
    "\n",
    "[usage]\n",
    "\ttutorial_4 [option]\n",
    "\toptions:\n",
    "\n",
    "\t\t-h                  Print a usage message\n",
    "\t\t-i <path/filename>  Required. Path to input video file, video elementary stream only\n",
    "\t\t-fr <val>           Number of frames from stream to process\n",
    "\t\t-m <path/filename>  Required. Path to IR .xml file.\n",
    "\t\t-l <path/filename>  Required. Path to labels file.\n",
    "\t\t-d <device>         Infer target device (CPU or GPU or MYRIAD)\n",
    "\t\t-pc                 Enables per-layer performance report\n",
    "\t\t-thresh <val>       Confidence threshold for bounding boxes 0-1\n",
    "\t\t-batch <val>        Batch size\n",
    "\t\t-s                  Display less information on the screen\n",
    "\t\t-e <path/filename>  Load layer extension plugin\n",
    "\t\t-mean               Mean value for normalization of data during planar BGR blob preprocess step\n",
    "\t\t-scale              Scale value for normalization of data during planar BGR blob preprocess step\n",
    "\n",
    "```\n",
    "\n",
    "As seen from above example, this application is able to load OpenVINO model and is able to run inference on CPU, GPU and MYRIAD. \n",
    "\n",
    "We will use run.sh file to run tutorial4 on CPU and run_gpu.sh to run inference on GPU.\n",
    "\n",
    "We see that there is a particular improvement on inference. \n",
    "\n",
    "```bash\n",
    "\n",
    "./run.sh\n",
    "\n",
    "Batch: 254/256\n",
    "\tpre-stage:\t3.79 ms/frame\n",
    "\tinfer:\t\t54.34 ms/frame\n",
    "\tpost-stage:\t0.40 ms/frame\n",
    "\n",
    "Batch: 255/256\n",
    "\tpre-stage:\t3.54 ms/frame\n",
    "\tinfer:\t\t52.75 ms/frame\n",
    "\tpost-stage:\t0.51 ms/frame\n",
    "\n",
    "Batch: 256/256\n",
    "\tpre-stage:\t6.91 ms/frame\n",
    "\tinfer:\t\t57.08 ms/frame\n",
    "\tpost-stage:\t0.42 ms/frame\n",
    "\n",
    "\n",
    "\n",
    "> Pre-stage average:\t4.97 ms/frame (decoding, color converting, resizing)\n",
    "> Infer average:\t55.67 ms/frame (inferencing)\n",
    "> Post-stage average:\t0.55 ms/frame (drawing bounding box, encoding, saving)\n",
    "\n",
    "> Total elapsed execution time: 15.83 sec\n",
    "\n",
    "> Done ! (Output is in out.h264 -> $ mplayer out.h264)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Let's run on GPU, this time IE loads clDNNPlugin. \n",
    "\n",
    "```bash\n",
    "\n",
    "./run_gpu.sh\n",
    "\n",
    "Batch: 254/256\n",
    "\tpre-stage:\t4.80 ms/frame\n",
    "\tinfer:\t\t39.26 ms/frame\n",
    "\tpost-stage:\t0.83 ms/frame\n",
    "\n",
    "Batch: 255/256\n",
    "\tpre-stage:\t7.55 ms/frame\n",
    "\tinfer:\t\t39.18 ms/frame\n",
    "\tpost-stage:\t1.60 ms/frame\n",
    "\n",
    "Batch: 256/256\n",
    "\tpre-stage:\t7.02 ms/frame\n",
    "\tinfer:\t\t38.16 ms/frame\n",
    "\tpost-stage:\t1.12 ms/frame\n",
    "\n",
    "\n",
    "\n",
    "> Pre-stage average:\t5.36 ms/frame (decoding, color converting, resizing)\n",
    "> Infer average:\t39.80 ms/frame (inferencing)\n",
    "> Post-stage average:\t0.69 ms/frame (drawing bounding box, encoding, saving)\n",
    "\n",
    "> Total elapsed execution time: 11.87 sec\n",
    "\n",
    "> Done ! (Output is in out.h264 -> $ mplayer out.h264)\n",
    "\n",
    "```\n",
    "\n",
    "Final output of inference written to out.h264 file, you can play it with mplayer and see the inference output.\n",
    "\n",
    "```bash\n",
    "mplayer out.h264\n",
    "```\n",
    "\n",
    "Let's get into details and investigate the perofmance little more.\n",
    "\n",
    "## Performance Analysis\n",
    "\n",
    "As we seen from the previous sections. We are able to get performance analysis. It is also implemented for this application. It will be a messy output to see each inference performance outputs.\n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "export LD_LIBRARY_PATH=/home/intel/inference_engine_samples/intel64/Release/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "/home/intel/Tutorials/interop_tutorials/tutorial_4/tutorial4 -m /home/intel/Tutorials/test_content/IR/SSD/SSD_GoogleNet_v2_fp16.xml -i /home/intel/Tutorials/test_content/video/cars_1920x1080.h264 -l /home/intel/Tutorials/test_content/IR/SSD/pascal_voc_classes.txt -d GPU -pc\n",
    "```\n",
    "\n",
    "## Batch Support\n",
    "\n",
    "Let's see how we can use a larger batch size with this example.\n",
    "\n",
    "At the output, you should realize that, it does show 32 inferences which means it packed 256 frames to batches and does inference at the all 8 same time. \n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "export LD_LIBRARY_PATH=/home/intel/inference_engine_samples/intel64/Release/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "/home/intel/Tutorials/interop_tutorials/tutorial_4/tutorial4 -m /home/intel/Tutorials/test_content/IR/SSD/SSD_GoogleNet_v2_fp16.xml -i /home/intel/Tutorials/test_content/video/cars_1920x1080.h264 -l /home/intel/Tutorials/test_content/IR/SSD/pascal_voc_classes.txt -d GPU -batch 8\n",
    "\n",
    "Batch: 30/32\n",
    "\tpre-stage:\t4.87 ms/frame\n",
    "\tinfer:\t\t22.76 ms/frame\n",
    "\tpost-stage:\t2.34 ms/frame\n",
    "\n",
    "Batch: 31/32\n",
    "\tpre-stage:\t4.74 ms/frame\n",
    "\tinfer:\t\t22.19 ms/frame\n",
    "\tpost-stage:\t2.23 ms/frame\n",
    "\n",
    "Batch: 32/32\n",
    "\tpre-stage:\t5.00 ms/frame\n",
    "\tinfer:\t\t22.05 ms/frame\n",
    "\tpost-stage:\t2.28 ms/frame\n",
    "\n",
    "\n",
    "\n",
    "> Pre-stage average:\t5.07 ms/frame (decoding, color converting, resizing)\n",
    "> Infer average:\t22.32 ms/frame (inferencing)\n",
    "> Post-stage average:\t2.29 ms/frame (drawing bounding box, encoding, saving)\n",
    "\n",
    "> Total elapsed execution time: 7.70 sec\n",
    "\n",
    "> Done ! (Output is in out.h264 -> $ mplayer out.h264)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5\n",
    "\n",
    "In Tutorial 5 application, we add Media SDK for decoding, pre-processing, post-processing and encoding steps let's see difference on the pre-stage.\n",
    "\n",
    "Let's open a new terminal. \n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "export LD_LIBRARY_PATH=/home/intel/inference_engine_samples/intel64/Release/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "cd /home/intel/Tutorials/interop_tutorials/tutorial_5\n",
    "\n",
    "./tutorial5 -h\n",
    "\n",
    "[usage]\n",
    "\ttutorial_5 [option]\n",
    "\toptions:\n",
    "\n",
    "\t\t-h                  Print a usage message\n",
    "\t\t-i <path/filename>  Required. Path to input video file, video elementary stream only\n",
    "\t\t-fr <val>           Number of frames from stream to process\n",
    "\t\t-m <path/filename>  Required. Path to IR .xml file.\n",
    "\t\t-l <path/filename>  Required. Path to labels file.\n",
    "\t\t-d <device>         Infer target device (CPU or GPU or MYRIAD)\n",
    "\t\t-pc                 Enables per-layer performance report\n",
    "\t\t-thresh <val>       Confidence threshold for bounding boxes 0-1\n",
    "\t\t-b <val>            Batch size\n",
    "\t\t-s                  Display less information on the screen\n",
    "\t\t-e <path/filename>  Load layer extension plugin\n",
    "\t\t-mean               Mean value for normalization of data during planar BGR blob preprocess step\n",
    "\t\t-scale              Scale value for normalization of data during planar BGR blob preprocess step\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "As seen from above example, this application is able to load OpenVINO model and is able to run inference on CPU, GPU and MYRIAD. \n",
    "\n",
    "Let's run on GPU, this time IE loads clDNNPlugin. \n",
    "\n",
    "```bash\n",
    "\n",
    "./run_gpu.sh\n",
    "\n",
    "Batch: 255/256\n",
    "\tpre-stage:\t6.05 ms/frame\n",
    "\tinfer:\t\t38.90 ms/frame\n",
    "\tpost-stage:\t2.40 ms/frame\n",
    "\n",
    "Batch: 256/256\n",
    "\tpre-stage:\t6.66 ms/frame\n",
    "\tinfer:\t\t37.13 ms/frame\n",
    "\tpost-stage:\t4.58 ms/frame\n",
    "\n",
    "Batch: 257/256\n",
    "\tpre-stage:\t6.17 ms/frame\n",
    "\n",
    "\n",
    "> Pre-stage average:\t5.97 ms/frame (decoding, color converting, resizing)\n",
    "> Infer average:\t42.21 ms/frame (inferencing)\n",
    "> Post-stage average:\t2.74 ms/frame (drawing bounding box, encoding, saving)\n",
    "\n",
    "> Total elapsed execution time: 13.08 sec\n",
    "\n",
    "Done!\n",
    "\n",
    "```\n",
    "\n",
    "Final output of inference written to out.h264 file, you can play it with mplayer and see the inference output.\n",
    "\n",
    "```bash\n",
    "mplayer out.h264\n",
    "```\n",
    "\n",
    "Let's get into details and investigate the perofmance little more.\n",
    "\n",
    "## Performance Analysis\n",
    "\n",
    "As we seen from the previous sections. We are able to get performance analysis. It is also implemented for this application. It will be a messy output to see each inference performance outputs.\n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "export LD_LIBRARY_PATH=/home/intel/inference_engine_samples/intel64/Release/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "/home/intel/Tutorials/interop_tutorials/tutorial_5/tutorial5 -m /home/intel/Tutorials/test_content/IR/SSD/SSD_GoogleNet_v2_fp16.xml -i /home/intel/Tutorials/test_content/video/cars_1920x1080.h264 -l /home/intel/Tutorials/test_content/IR/SSD/pascal_voc_classes.txt -d GPU -pc\n",
    "```\n",
    "\n",
    "## Batch Support\n",
    "\n",
    "Let's see how we can use a larger batch size with this example.\n",
    "\n",
    "At the output, you should realize that, it does show 32 inferences which means it packed 256 frames to batches and does inference at the all 8 same time. \n",
    "\n",
    "```bash\n",
    "source /opt/intel/computer_vision_sdk/bin/setupvars.sh\n",
    "export LD_LIBRARY_PATH=/home/intel/inference_engine_samples/intel64/Release/lib:$LD_LIBRARY_PATH\n",
    "\n",
    "/home/intel/Tutorials/interop_tutorials/tutorial_5/tutorial5 -m /home/intel/Tutorials/test_content/IR/SSD/SSD_GoogleNet_v2_fp16.xml -i /home/intel/Tutorials/test_content/video/cars_1920x1080.h264 -l /home/intel/Tutorials/test_content/IR/SSD/pascal_voc_classes.txt -d GPU -batch 8\n",
    "\n",
    "Batch: 31/32\n",
    "\tpre-stage:\t7.95 ms/frame\n",
    "\tinfer:\t\t22.76 ms/frame\n",
    "\tpost-stage:\t5.50 ms/frame\n",
    "\n",
    "Batch: 32/32\n",
    "\tpre-stage:\t8.23 ms/frame\n",
    "\tinfer:\t\t22.83 ms/frame\n",
    "\tpost-stage:\t5.95 ms/frame\n",
    "\n",
    "Batch: 33/32\n",
    "\tpre-stage:\t6.54 ms/frame\n",
    "\n",
    "\n",
    "> Pre-stage average:\t6.81 ms/frame (decoding, color converting, resizing)\n",
    "> Infer average:\t22.88 ms/frame (inferencing)\n",
    "> Post-stage average:\t6.16 ms/frame (drawing bounding box, encoding, saving)\n",
    "\n",
    "> Total elapsed execution time: 9.26 sec\n",
    "\n",
    "Done!\n",
    "\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
